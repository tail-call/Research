%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Flickering Glow at 2025-04-25 10:35:15 +0700 


%% Saved with string encoding Unicode (UTF-8) 



@article{dlpl,
	author = {Geoffrey S. H. Cruttwell and Bruno Gavranovi{\'c}, Neil Ghani and Paul Wilson and Fabio Zanasi},
	date-added = {2025-03-26 14:45:39 +0700},
	date-modified = {2025-03-26 14:48:19 +0700},
	month = {Mar},
	title = {Deep Learning with Parametric Lenses},
	year = {2024},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEDguLi9TdHVkeS9QYXBlcnMvRGVlcCBMZWFybmluZyB3aXRoIFBhcmFtZXRyaWMgTGVuc2VzLnBkZk8RA/Bib29r8AMAAAAABBAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADsAgAABQAAAAEBAABVc2VycwAAAAYAAAABAQAAc2NhbGVzAAAFAAAAAQEAAFN0dWR5AAAABgAAAAEBAABQYXBlcnMAACgAAAABAQAARGVlcCBMZWFybmluZyB3aXRoIFBhcmFtZXRyaWMgTGVuc2VzLnBkZhQAAAABBgAABAAAABQAAAAkAAAANAAAAEQAAAAIAAAABAMAAKVBAAAAAAAACAAAAAQDAAAp2gUAAAAAAAgAAAAEAwAAK5gkAAAAAAAIAAAABAMAAHPHXwAAAAAACAAAAAQDAADfXD8EAAAAABQAAAABBgAAkAAAAKAAAACwAAAAwAAAANAAAAAIAAAAAAQAAEHGyfQDdMKSGAAAAAECAAABAAAAAAAAAA8AAAAAAAAAAAAAAAAAAAAIAAAABAMAAAMAAAAAAAAABAAAAAMDAAD3AQAACAAAAAEJAABmaWxlOi8vLwwAAAABAQAATWFjaW50b3NoIEhECAAAAAQDAAAAkIKW5wAAAAgAAAAABAAAQca81YUAAAAkAAAAAQEAADQzODc2NTg0LTRCNjgtNEVBMy1CMDAyLUU0NEE0QjkyQUQ0QxgAAAABAgAAgQAAAAEAAADvEwAAAQAAAAAAAAAAAAAAAQAAAAEBAAAvAAAAAAAAAAEFAAD1AAAAAQIAADFkOGFjN2VlMDRjMTgzOWM1MGFmMGQ1NzNiNGE4OTEyYzAyMzA5OTZkODllZGJkMjJhNmE4MzU0YTkxNmVkZmU7MDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDAwMDAwMDAyMDtjb20uYXBwbGUuYXBwLXNhbmRib3gucmVhZC13cml0ZTswMTswMTAwMDAwZjswMDAwMDAwMDA0M2Y1Y2RmOzAxOy91c2Vycy9zY2FsZXMvc3R1ZHkvcGFwZXJzL2RlZXAgbGVhcm5pbmcgd2l0aCBwYXJhbWV0cmljIGxlbnNlcy5wZGYAAAAAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAdAAAAAAAAAAFEAAA4AAAAAAAAAAQEAAADAEAAAAAAABAEAAA/AAAAAAAAAACIAAA2AEAAAAAAAAFIAAASAEAAAAAAAAQIAAAWAEAAAAAAAARIAAAjAEAAAAAAAASIAAAbAEAAAAAAAATIAAAfAEAAAAAAAAgIAAAuAEAAAAAAAAwIAAA5AEAAAAAAAABwAAALAEAAAAAAAARwAAAFAAAAAAAAAASwAAAPAEAAAAAAACA8AAA7AEAAAAAAAAACAANABoAIwBeAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABFI=}}

@inproceedings{mckinney-proc-scipy-2010,
	author = {{W}es {M}c{K}inney},
	booktitle = {{P}roceedings of the 9th {P}ython in {S}cience {C}onference},
	date-added = {2025-03-10 15:06:49 +0700},
	date-modified = {2025-03-10 15:06:49 +0700},
	doi = {10.25080/Majora-92bf1922-00a},
	editor = {{S}t\'efan van der {W}alt and {J}arrod {M}illman},
	pages = {56 - 61},
	title = {{D}ata {S}tructures for {S}tatistical {C}omputing in {P}ython},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.25080/Majora-92bf1922-00a}}

@article{scikit-learn,
	author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	date-added = {2025-03-10 15:04:05 +0700},
	date-modified = {2025-03-10 15:04:05 +0700},
	journal = {Journal of Machine Learning Research},
	pages = {2825--2830},
	title = {Scikit-learn: Machine Learning in {P}ython},
	volume = {12},
	year = {2011}}

@article{sevensketches,
	author = {Brendan Fong and David I. Spivak},
	date-added = {2025-03-09 23:36:16 +0700},
	date-modified = {2025-03-09 23:37:38 +0700},
	doi = {10.48550/arXiv.1803.05316},
	month = {03},
	title = {Seven Sketches in Compositionality: An Invitation to Applied Category Theory},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.1803.05316}}

@article{leelim20,
	author = {Changhun Lee and Chiehyeon Lim},
	date-added = {2025-02-15 18:50:01 +0700},
	date-modified = {2025-02-15 20:14:27 +0700},
	rating = {4},
	read = {1},
	title = {Reward Dropout Improves Control: Bi-objective Perspective on Reinforced LM},
	year = {2023},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEF4uLi9TdHVkeS9QYXBlcnMvUmV3YXJkIERyb3BvdXQgSW1wcm92ZXMgQ29udHJvbC0gQmktb2JqZWN0aXZlIFBlcnNwZWN0aXZlIG9uIFJlaW5mb3JjZWQgTE0ucGRmTxEEPGJvb2s8BAAAAAAEEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADgDAAAFAAAAAQEAAFVzZXJzAAAABgAAAAEBAABzY2FsZXMAAAUAAAABAQAAU3R1ZHkAAAAGAAAAAQEAAFBhcGVycwAATgAAAAEBAABSZXdhcmQgRHJvcG91dCBJbXByb3ZlcyBDb250cm9sLSBCaS1vYmplY3RpdmUgUGVyc3BlY3RpdmUgb24gUmVpbmZvcmNlZCBMTS5wZGYAABQAAAABBgAABAAAABQAAAAkAAAANAAAAEQAAAAIAAAABAMAAKVBAAAAAAAACAAAAAQDAAAp2gUAAAAAAAgAAAAEAwAAK5gkAAAAAAAIAAAABAMAAHPHXwAAAAAACAAAAAQDAADoIo8CAAAAABQAAAABBgAAuAAAAMgAAADYAAAA6AAAAPgAAAAIAAAAAAQAAEHGsFujuGq3GAAAAAECAAABAAAAAAAAAA8AAAAAAAAAAAAAAAAAAAAIAAAABAMAAAMAAAAAAAAABAAAAAMDAAD3AQAACAAAAAEJAABmaWxlOi8vLwwAAAABAQAATWFjaW50b3NoIEhECAAAAAQDAAAAkIKW5wAAAAgAAAAABAAAQca81YUAAAAkAAAAAQEAADQzODc2NTg0LTRCNjgtNEVBMy1CMDAyLUU0NEE0QjkyQUQ0QxgAAAABAgAAgQAAAAEAAADvEwAAAQAAAAAAAAAAAAAAAQAAAAEBAAAvAAAAAAAAAAEFAAAbAQAAAQIAAGRkOGU4MDc5ZDQwMTZkMjk3MzUzNmM3YTMyY2UwMGYxZDRkNTk2MjBkNDAwNWU2ODJmOTlmMGIwZTQ1NjhiZmQ7MDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDAwMDAwMDAyMDtjb20uYXBwbGUuYXBwLXNhbmRib3gucmVhZC13cml0ZTswMTswMTAwMDAwZjswMDAwMDAwMDAyOGYyMmU4OzAxOy91c2Vycy9zY2FsZXMvc3R1ZHkvcGFwZXJzL3Jld2FyZCBkcm9wb3V0IGltcHJvdmVzIGNvbnRyb2wtIGJpLW9iamVjdGl2ZSBwZXJzcGVjdGl2ZSBvbiByZWluZm9yY2VkIGxtLnBkZgAAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAnAAAAAAAAAAFEAAACAEAAAAAAAAQEAAANAEAAAAAAABAEAAAJAEAAAAAAAACIAAAAAIAAAAAAAAFIAAAcAEAAAAAAAAQIAAAgAEAAAAAAAARIAAAtAEAAAAAAAASIAAAlAEAAAAAAAATIAAApAEAAAAAAAAgIAAA4AEAAAAAAAAwIAAADAIAAAAAAAABwAAAVAEAAAAAAAARwAAAFAAAAAAAAAASwAAAZAEAAAAAAACA8AAAFAIAAAAAAAAACAANABoAIwCEAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABMQ=}}

@inproceedings{tseng20,
	abstract = {With the growing attention on learning-to-learn new tasks using only a few examples, meta-learning has been widely used in numerous problems such as few-shot classification, reinforcement learning, and domain generalization. However, meta-learning models are prone to overfitting when there are no sufficient training tasks for the meta-learners to generalize. Although existing approaches such as Dropout are widely used to address the overfitting problem, these methods are typically designed for regularizing models of a single task in supervised training. In this paper, we introduce a simple yet effective method to alleviate the risk of overfitting for gradient-based meta-learning. Specifically, during the gradient-based adaptation stage, we randomly drop the gradient in the inner-loop optimization of each parameter in deep neural networks, such that the augmented gradients improve generalization to new tasks. We present a general form of the proposed gradient dropout regularization and show that this term can be sampled from either the Bernoulli or Gaussian distribution. To validate the proposed method, we conduct extensive experiments and analysis on numerous computer vision tasks, demonstrating that the gradient dropout regularization mitigates the overfitting problem and improves the performance upon various gradient-based meta-learning frameworks.},
	author = {Hung-Yu Tseng and Yi-Wen Chen and Yi-Hsuan Tsai and Sifei Liu and Yen-Yu Lin and Ming-Hsuan Yang},
	booktitle = {Computer Vision -- ACCV 2020. 15th Asian Conference on Computer Vision, Kyoto, Japan, November 30 -- December 4, 2020, Revised Selected Papers, Part IV},
	date-added = {2025-02-10 15:26:59 +0700},
	date-modified = {2025-02-12 20:22:54 +0700},
	doi = {https://doi.org/10.48550/arXiv.2004.05859},
	editor = {Hiroshi Ishikawa and Cheng-Lin Liu and Tomas Pajdla and Jianbo Shi},
	isbn = {978-3-030-69537-8},
	keywords = {meta-learning, dropout, regularization, Bernoulli distribution},
	month = {December},
	pages = {218--314},
	publisher = {Springer Cham},
	rating = {4},
	read = {1},
	title = {Regularizing Meta-Learning via Gradient Dropout},
	url = {https://arxiv.org/abs/2004.05859},
	url2 = {https://link.springer.com/chapter/10.1007/978-3-030-69538-5_14},
	year = {2020},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEMuLi9TdHVkeS9QYXBlcnMvUmVndWxhcml6aW5nIE1ldGEtTGVhcm5pbmcgdmlhIEdyYWRpZW50IERyb3BvdXQucGRmTxEEBGJvb2sEBAAAAAAEEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAAAFAAAAAQEAAFVzZXJzAAAABgAAAAEBAABzY2FsZXMAAAUAAAABAQAAU3R1ZHkAAAAGAAAAAQEAAFBhcGVycwAAMwAAAAEBAABSZWd1bGFyaXppbmcgTWV0YS1MZWFybmluZyB2aWEgR3JhZGllbnQgRHJvcG91dC5wZGYAFAAAAAEGAAAEAAAAFAAAACQAAAA0AAAARAAAAAgAAAAEAwAApUEAAAAAAAAIAAAABAMAACnaBQAAAAAACAAAAAQDAAArmCQAAAAAAAgAAAAEAwAAc8dfAAAAAAAIAAAABAMAAFrphQIAAAAAFAAAAAEGAACcAAAArAAAALwAAADMAAAA3AAAAAgAAAAABAAAQcas997ykmQYAAAAAQIAAAEAAAAAAAAADwAAAAAAAAAAAAAAAAAAAAgAAAAEAwAAAwAAAAAAAAAEAAAAAwMAAPcBAAAIAAAAAQkAAGZpbGU6Ly8vDAAAAAEBAABNYWNpbnRvc2ggSEQIAAAABAMAAACQgpbnAAAACAAAAAAEAABBxrzVhQAAACQAAAABAQAANDM4NzY1ODQtNEI2OC00RUEzLUIwMDItRTQ0QTRCOTJBRDRDGAAAAAECAACBAAAAAQAAAO8TAAABAAAAAAAAAAAAAAABAAAAAQEAAC8AAAAAAAAAAQUAAAABAAABAgAANzQyNWY5MTliNzMwMWE0ODIzNjE5YWYxMjBkNGRmN2M1NDc3MzBkYmY2MjE5MTI3YzhlNmVhNTNhMGVkZjhjNDswMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDAwMDAwMDIwO2NvbS5hcHBsZS5hcHAtc2FuZGJveC5yZWFkLXdyaXRlOzAxOzAxMDAwMDBmOzAwMDAwMDAwMDI4NWU5NWE7MDE7L3VzZXJzL3NjYWxlcy9zdHVkeS9wYXBlcnMvcmVndWxhcml6aW5nIG1ldGEtbGVhcm5pbmcgdmlhIGdyYWRpZW50IGRyb3BvdXQucGRmAMwAAAD+////AQAAAAAAAAAQAAAABBAAAIAAAAAAAAAABRAAAOwAAAAAAAAAEBAAABgBAAAAAAAAQBAAAAgBAAAAAAAAAiAAAOQBAAAAAAAABSAAAFQBAAAAAAAAECAAAGQBAAAAAAAAESAAAJgBAAAAAAAAEiAAAHgBAAAAAAAAEyAAAIgBAAAAAAAAICAAAMQBAAAAAAAAMCAAAPABAAAAAAAAAcAAADgBAAAAAAAAEcAAABQAAAAAAAAAEsAAAEgBAAAAAAAAgPAAAPgBAAAAAAAAAAgADQAaACMAaQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAARx}}

@inproceedings{hein17,
	abstract = {Recent work has shown that state-of-the-art classifiers are quite brittle,
in the sense that a small adversarial change of an originally with high
confidence correctly classified input leads to a wrong classification again
with high confidence. This raises concerns that such classifiers are vulnerable
to attacks and calls into question their usage in safety-critical systems. We
show in this paper for the first time formal guarantees on the robustness
of a classifier by giving instance-specific lower bounds on the norm of the
input manipulation required to change the classifier decision. Based on
this analysis we propose the Cross-Lipschitz regularization functional. We
show that using this form of regularization in kernel methods resp. neural
networks improves the robustness of the classifier with no or small loss in
prediction performance.},
	author = {Matthias Hein and Maksym Andriushchenko},
	booktitle = {31st Conference on Neural Information Processing Systems},
	date-added = {2025-01-31 03:09:51 +0700},
	date-modified = {2025-01-31 03:12:22 +0700},
	title = {Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation},
	year = {2017},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEGguLi9TdHVkeS9QYXBlcnMvRm9ybWFsIEd1YXJhbnRlZXMgb24gdGhlIFJvYnVzdG5lc3Mgb2YgYSBDbGFzc2lmaWVyIGFnYWluc3QgQWR2ZXJzYXJpYWwgTWFuaXB1bGF0aW9uLnBkZk8RBFBib29rUAQAAAAABBAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABMAwAABQAAAAEBAABVc2VycwAAAAYAAAABAQAAc2NhbGVzAAAFAAAAAQEAAFN0dWR5AAAABgAAAAEBAABQYXBlcnMAAFgAAAABAQAARm9ybWFsIEd1YXJhbnRlZXMgb24gdGhlIFJvYnVzdG5lc3Mgb2YgYSBDbGFzc2lmaWVyIGFnYWluc3QgQWR2ZXJzYXJpYWwgTWFuaXB1bGF0aW9uLnBkZhQAAAABBgAABAAAABQAAAAkAAAANAAAAEQAAAAIAAAABAMAAKVBAAAAAAAACAAAAAQDAAAp2gUAAAAAAAgAAAAEAwAAK5gkAAAAAAAIAAAABAMAAHPHXwAAAAAACAAAAAQDAAATnW0CAAAAABQAAAABBgAAwAAAANAAAADgAAAA8AAAAAABAAAIAAAAAAQAAEHGpgn9OpxwGAAAAAECAAABAAAAAAAAAA8AAAAAAAAAAAAAAAAAAAAIAAAABAMAAAMAAAAAAAAABAAAAAMDAAD3AQAACAAAAAEJAABmaWxlOi8vLwwAAAABAQAATWFjaW50b3NoIEhECAAAAAQDAAAAkIKW5wAAAAgAAAAABAAAQca81YUAAAAkAAAAAQEAADQzODc2NTg0LTRCNjgtNEVBMy1CMDAyLUU0NEE0QjkyQUQ0QxgAAAABAgAAgQAAAAEAAADvEwAAAQAAAAAAAAAAAAAAAQAAAAEBAAAvAAAAAAAAAAEFAAAlAQAAAQIAAGM2ZjM0NjE4ZmY3NDk1MzZiZmMxN2RlZTU4ZjE2NjM0YmYyMDE3ODZkOTI3ZTI5MDkyZmE0N2Q1NjZmZTcyOTM7MDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDAwMDAwMDAyMDtjb20uYXBwbGUuYXBwLXNhbmRib3gucmVhZC13cml0ZTswMTswMTAwMDAwZjswMDAwMDAwMDAyNmQ5ZDEzOzAxOy91c2Vycy9zY2FsZXMvc3R1ZHkvcGFwZXJzL2Zvcm1hbCBndWFyYW50ZWVzIG9uIHRoZSByb2J1c3RuZXNzIG9mIGEgY2xhc3NpZmllciBhZ2FpbnN0IGFkdmVyc2FyaWFsIG1hbmlwdWxhdGlvbi5wZGYAAAAAzAAAAP7///8BAAAAAAAAABAAAAAEEAAApAAAAAAAAAAFEAAAEAEAAAAAAAAQEAAAPAEAAAAAAABAEAAALAEAAAAAAAACIAAACAIAAAAAAAAFIAAAeAEAAAAAAAAQIAAAiAEAAAAAAAARIAAAvAEAAAAAAAASIAAAnAEAAAAAAAATIAAArAEAAAAAAAAgIAAA6AEAAAAAAAAwIAAAFAIAAAAAAAABwAAAXAEAAAAAAAARwAAAFAAAAAAAAAASwAAAbAEAAAAAAACA8AAAHAIAAAAAAAAACAANABoAIwCOAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABOI=}}

@article{Jr:2023aa,
	abstract = {We show how solution concepts from cooperative game theory can be used to tackle the problem of pruning neural networks. 
The ever-growing size of deep neural networks (DNNs) increases their performance, but also their computational requirements. We introduce a method called Game Theory Assisted Pruning (GTAP), which reduces the neural network's size while preserving its predictive accuracy. GTAP is based on eliminating neurons in the network based on an estimation of their joint impact on the prediction quality through game theoretic solutions. Specifically, we use a power index akin to the Shapley value or Banzhaf index, tailored using a procedure similar to Dropout (commonly used to tackle overfitting problems in machine learning). 
Empirical evaluation of both feedforward networks and convolutional neural networks shows that this method outperforms existing approaches in the achieved tradeoff between the number of parameters and model accuracy.},
	author = {Mauricio Diaz-Ortiz Jr and Benjamin Kempinski and Daphne Cornelisse and Yoram Bachrach and Tal Kachman},
	date-added = {2025-01-29 12:10:41 +0700},
	date-modified = {2025-01-29 12:10:51 +0700},
	eprint = {2311.10468},
	month = {Nov},
	title = {Using Cooperative Game Theory to Prune Neural Networks},
	url = {https://arxiv.org/pdf/2311.10468.pdf},
	year = {2023},
	bdsk-url-1 = {https://arxiv.org/pdf/2311.10468.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/2311.10468}}

@article{Box_1958,
	author = {Box, G. E. P. and Muller, Mervin E.},
	date-added = {2025-01-18 13:26:22 +0700},
	date-modified = {2025-01-18 13:26:37 +0700},
	doi = {10.1214/aoms/1177706645},
	issn = {0003-4851},
	journal = {The Annals of Mathematical Statistics},
	month = jun,
	number = {2},
	pages = {610--611},
	publisher = {Institute of Mathematical Statistics},
	rating = {5},
	title = {A Note on the Generation of Random Normal Deviates},
	url = {http://dx.doi.org/10.1214/aoms/1177706645},
	volume = {29},
	year = {1958},
	bdsk-url-1 = {http://dx.doi.org/10.1214/aoms/1177706645}}

@article{romano2021pmlb,
	abstract = {Bioinformatics:

MOTIVATION
Novel machine learning and statistical modeling studies rely on standardized comparisons to existing methods using well-studied benchmark datasets. Few tools exist that provide rapid access to many of these datasets through a standardized, user-friendly interface that integrates well with popular data science workflows.

RESULTS
This release of PMLB (Penn Machine Learning Benchmarks) provides the largest collection of diverse, public benchmark datasets for evaluating new machine learning and data science methods aggregated in one location. v1.0 introduces a number of critical improvements developed following discussions with the open-source community.

AVAILABILITY AND IMPLEMENTATION
PMLB is available at https://github.com/EpistasisLab/pmlb. Python and R interfaces for PMLB can be installed through the Python Package Index and Comprehensive R Archive Network, respectively.



ArXiV:

Background
The selection, development, or comparison of machine learning methods in data mining can be a difficult task based on the target problem and goals of a particular study. Numerous publicly available real-world and simulated benchmark datasets have emerged from different sources, but their organization and adoption as standards have been inconsistent. As such, selecting and curating specific benchmarks remains an unnecessary burden on machine learning practitioners and data scientists.

Results
The present study introduces an accessible, curated, and developing public benchmark resource to facilitate identification of the strengths and weaknesses of different machine learning methodologies. We compare meta-features among the current set of benchmark datasets in this resource to characterize the diversity of available data. Finally, we apply a number of established machine learning methods to the entire benchmark suite and analyze how datasets and algorithms cluster in terms of performance. From this study, we find that existing benchmarks lack the diversity to properly benchmark machine learning algorithms, and there are several gaps in benchmarking problems that still need to be considered.

Conclusions
This work represents another important step towards understanding the limitations of popular benchmarking suites and developing a resource that connects existing benchmarking standards to more diverse and efficient standards in the future.

},
	arxiv = {arXiv:2012.00058v2},
	author = {Romano, Joseph D and Le, Trang T and La Cava, William and Gregg, John T and Goldberg, Daniel J and Chakraborty, Praneel and Ray, Natasha L and Himmelstein, Daniel and Fu, Weixuan and Moore, Jason H},
	date-modified = {2025-02-12 21:16:19 +0700},
	journal = {Bioinformatics},
	month = {Jan},
	number = {3},
	pages = {878--880},
	rating = {4},
	read = {1},
	title = {PMLB v1.0: an open source dataset collection for benchmarking machine learning methods},
	url = {https://www.unboundmedicine.com/medline/citation/34677586/PMLB_v1_0:_an_open_source_dataset_collection_for_benchmarking_machine_learning_methods_},
	volume = {38},
	year = {2022},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEGEuLi9TdHVkeS9QYXBlcnMvUE1MQiAtIGEgbGFyZ2UgYmVuY2htYXJrIHN1aXRlIGZvciBtYWNoaW5lIGxlYXJuaW5nIGV2YWx1YXRpb24gYW5kIGNvbXBhcmlzb24ucGRmTxEERGJvb2tEBAAAAAAEEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEADAAAFAAAAAQEAAFVzZXJzAAAABgAAAAEBAABzY2FsZXMAAAUAAAABAQAAU3R1ZHkAAAAGAAAAAQEAAFBhcGVycwAAUQAAAAEBAABQTUxCIC0gYSBsYXJnZSBiZW5jaG1hcmsgc3VpdGUgZm9yIG1hY2hpbmUgbGVhcm5pbmcgZXZhbHVhdGlvbiBhbmQgY29tcGFyaXNvbi5wZGYAAAAUAAAAAQYAAAQAAAAUAAAAJAAAADQAAABEAAAACAAAAAQDAAClQQAAAAAAAAgAAAAEAwAAKdoFAAAAAAAIAAAABAMAACuYJAAAAAAACAAAAAQDAABzx18AAAAAAAgAAAAEAwAAlWhIAgAAAAAUAAAAAQYAALwAAADMAAAA3AAAAOwAAAD8AAAACAAAAAAEAABBxppnXEjiaxgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAADAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAJCClucAAAAIAAAAAAQAAEHGvNWFAAAAJAAAAAEBAAA0Mzg3NjU4NC00QjY4LTRFQTMtQjAwMi1FNDRBNEI5MkFENEMYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAAHgEAAAECAAAwZmJhYWQ2NTA3MTJkOGY3NTUyYTVhOTA3MzY1NzVmYzczNTk5YTQ0ZjY4ZDUxODMyNDBlZWU1MDJhZGVkYjA5OzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMGY7MDAwMDAwMDAwMjQ4Njg5NTswMTsvdXNlcnMvc2NhbGVzL3N0dWR5L3BhcGVycy9wbWxiIC0gYSBsYXJnZSBiZW5jaG1hcmsgc3VpdGUgZm9yIG1hY2hpbmUgbGVhcm5pbmcgZXZhbHVhdGlvbiBhbmQgY29tcGFyaXNvbi5wZGYAAADMAAAA/v///wEAAAAAAAAAEAAAAAQQAACgAAAAAAAAAAUQAAAMAQAAAAAAABAQAAA4AQAAAAAAAEAQAAAoAQAAAAAAAAIgAAAEAgAAAAAAAAUgAAB0AQAAAAAAABAgAACEAQAAAAAAABEgAAC4AQAAAAAAABIgAACYAQAAAAAAABMgAACoAQAAAAAAACAgAADkAQAAAAAAADAgAAAQAgAAAAAAAAHAAABYAQAAAAAAABHAAAAUAAAAAAAAABLAAABoAQAAAAAAAIDwAAAYAgAAAAAAAAAIAA0AGgAjAIcAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAAEzw==}}

@book{mml-book,
	author = {Marc Peter Deisenroth and A. Aldo Faisal and Cheng Soon Ong},
	date-added = {2025-01-12 21:17:40 +0700},
	date-modified = {2025-01-12 21:22:21 +0700},
	keywords = {machine learning, textbook,},
	publisher = {Cambridge University Press},
	rating = {5},
	title = {Mathematics for Machine Learning},
	url = {https://mml-book.com},
	year = {2024},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEBwuLi9TdHVkeS9QYXBlcnMvbW1sLWJvb2sucGRmTxEDuGJvb2u4AwAAAAAEEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALQCAAAFAAAAAQEAAFVzZXJzAAAABgAAAAEBAABzY2FsZXMAAAUAAAABAQAAU3R1ZHkAAAAGAAAAAQEAAFBhcGVycwAADAAAAAEBAABtbWwtYm9vay5wZGYUAAAAAQYAAAQAAAAUAAAAJAAAADQAAABEAAAACAAAAAQDAAClQQAAAAAAAAgAAAAEAwAAKdoFAAAAAAAIAAAABAMAACuYJAAAAAAACAAAAAQDAABzx18AAAAAAAgAAAAEAwAAQiI3AgAAAAAUAAAAAQYAAHQAAACEAAAAlAAAAKQAAAC0AAAACAAAAAAEAABBxgMUF2TI1xgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAADAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAJCClucAAAAIAAAAAAQAAEHGgh1lgAAAJAAAAAEBAAA0Mzg3NjU4NC00QjY4LTRFQTMtQjAwMi1FNDRBNEI5MkFENEMYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAA2QAAAAECAABiZjRkNjQ3ZTk2MmQ1N2ViNDRkYmI0OWNiOTUyNWFhN2Y4OTYwNzQ2NTVmODE0ZGM1ZGFkNDMxMjk2MjRkMzBjOzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMGU7MDAwMDAwMDAwMjM3MjI0MjswMTsvdXNlcnMvc2NhbGVzL3N0dWR5L3BhcGVycy9tbWwtYm9vay5wZGYAAAAAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAWAAAAAAAAAAFEAAAxAAAAAAAAAAQEAAA8AAAAAAAAABAEAAA4AAAAAAAAAACIAAAvAEAAAAAAAAFIAAALAEAAAAAAAAQIAAAPAEAAAAAAAARIAAAcAEAAAAAAAASIAAAUAEAAAAAAAATIAAAYAEAAAAAAAAgIAAAnAEAAAAAAAAwIAAAyAEAAAAAAAABwAAAEAEAAAAAAAARwAAAFAAAAAAAAAASwAAAIAEAAAAAAACA8AAA0AEAAAAAAAAACAANABoAIwBCAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAA/4=}}

@article{reptile2018,
	abstract = {This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.},
	author = {Alex Nichol and Joshua Achiam and John Schulman},
	date-added = {2025-01-03 20:39:26 +0700},
	date-modified = {2025-01-03 20:52:39 +0700},
	eprint = {1803.02999},
	keywords = {meta-learning, parameter initialization, few-shot learning},
	month = {March},
	title = {On First-Order Meta-Learning Algorithms},
	url = {https://arxiv.org/pdf/1803.02999.pdf},
	year = {2018},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEDsuLi9TdHVkeS9QYXBlcnMvT24gRmlyc3QtT3JkZXIgTWV0YS1MZWFybmluZyBBbGdvcml0aG1zLnBkZk8RA/Rib29r9AMAAAAABBAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwAgAABQAAAAEBAABVc2VycwAAAAYAAAABAQAAc2NhbGVzAAAFAAAAAQEAAFN0dWR5AAAABgAAAAEBAABQYXBlcnMAACsAAAABAQAAT24gRmlyc3QtT3JkZXIgTWV0YS1MZWFybmluZyBBbGdvcml0aG1zLnBkZgAUAAAAAQYAAAQAAAAUAAAAJAAAADQAAABEAAAACAAAAAQDAAClQQAAAAAAAAgAAAAEAwAAKdoFAAAAAAAIAAAABAMAACuYJAAAAAAACAAAAAQDAABzx18AAAAAAAgAAAAEAwAAqrE6AgAAAAAUAAAAAQYAAJQAAACkAAAAtAAAAMQAAADUAAAACAAAAAAEAABBxpQQGq/qHxgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAADAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAJCClucAAAAIAAAAAAQAAEHGvNWFAAAAJAAAAAEBAAA0Mzg3NjU4NC00QjY4LTRFQTMtQjAwMi1FNDRBNEI5MkFENEMYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAA+AAAAAECAAAxNDQwZGQ4ZTk0NTAzNjBjMDJjMDZmNjhiZjliMmRhY2IwYzdhNGI1MTcxYWEwNWRhYzI1MWRjNDgzMjBmOGQ1OzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMGY7MDAwMDAwMDAwMjNhYjFhYTswMTsvdXNlcnMvc2NhbGVzL3N0dWR5L3BhcGVycy9vbiBmaXJzdC1vcmRlciBtZXRhLWxlYXJuaW5nIGFsZ29yaXRobXMucGRmAMwAAAD+////AQAAAAAAAAAQAAAABBAAAHgAAAAAAAAABRAAAOQAAAAAAAAAEBAAABABAAAAAAAAQBAAAAABAAAAAAAAAiAAANwBAAAAAAAABSAAAEwBAAAAAAAAECAAAFwBAAAAAAAAESAAAJABAAAAAAAAEiAAAHABAAAAAAAAEyAAAIABAAAAAAAAICAAALwBAAAAAAAAMCAAAOgBAAAAAAAAAcAAADABAAAAAAAAEcAAABQAAAAAAAAAEsAAAEABAAAAAAAAgPAAAPABAAAAAAAAAAgADQAaACMAYQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAARZ},
	bdsk-url-1 = {https://arxiv.org/abs/1803.02999}}

@article{maml2017,
	abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
	author = {Chelsea Finn and Pieter Abbeel and Sergey Levine},
	date-added = {2024-12-26 15:13:08 +0700},
	date-modified = {2025-01-03 19:07:26 +0700},
	eprint = {1703.03400},
	keywords = {meta-learning, few-shot learning, deep learning, deep neural networks, image classification},
	month = {March},
	rating = {5},
	read = {1},
	title = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
	url = {https://arxiv.org/pdf/1703.03400.pdf},
	year = {2017},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEFUuLi9TdHVkeS9QYXBlcnMvTW9kZWwtQWdub3N0aWMgTWV0YS1MZWFybmluZyBmb3IgRmFzdCBBZGFwdGF0aW9uIG9mIERlZXAgTmV0d29ya3MucGRmTxEELGJvb2ssBAAAAAAEEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACgDAAAFAAAAAQEAAFVzZXJzAAAABgAAAAEBAABzY2FsZXMAAAUAAAABAQAAU3R1ZHkAAAAGAAAAAQEAAFBhcGVycwAARQAAAAEBAABNb2RlbC1BZ25vc3RpYyBNZXRhLUxlYXJuaW5nIGZvciBGYXN0IEFkYXB0YXRpb24gb2YgRGVlcCBOZXR3b3Jrcy5wZGYAAAAUAAAAAQYAAAQAAAAUAAAAJAAAADQAAABEAAAACAAAAAQDAAClQQAAAAAAAAgAAAAEAwAAKdoFAAAAAAAIAAAABAMAACuYJAAAAAAACAAAAAQDAABzx18AAAAAAAgAAAAEAwAAvUI6AgAAAAAUAAAAAQYAALAAAADAAAAA0AAAAOAAAADwAAAACAAAAAAEAABBxo6jwG1PwxgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAADAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAJCClucAAAAIAAAAAAQAAEHGvNWFAAAAJAAAAAEBAAA0Mzg3NjU4NC00QjY4LTRFQTMtQjAwMi1FNDRBNEI5MkFENEMYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAAEgEAAAECAAA3MmEzMDYzYzA4OWVjMzc5NzExZWEzMTYyYjExNzdjOTA2YTdmOTBhMjMxMGEwNDk5N2FhZjBmMjA4NWRjMTIzOzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMGY7MDAwMDAwMDAwMjNhNDJiZDswMTsvdXNlcnMvc2NhbGVzL3N0dWR5L3BhcGVycy9tb2RlbC1hZ25vc3RpYyBtZXRhLWxlYXJuaW5nIGZvciBmYXN0IGFkYXB0YXRpb24gb2YgZGVlcCBuZXR3b3Jrcy5wZGYAAADMAAAA/v///wEAAAAAAAAAEAAAAAQQAACUAAAAAAAAAAUQAAAAAQAAAAAAABAQAAAsAQAAAAAAAEAQAAAcAQAAAAAAAAIgAAD4AQAAAAAAAAUgAABoAQAAAAAAABAgAAB4AQAAAAAAABEgAACsAQAAAAAAABIgAACMAQAAAAAAABMgAACcAQAAAAAAACAgAADYAQAAAAAAADAgAAAEAgAAAAAAAAHAAABMAQAAAAAAABHAAAAUAAAAAAAAABLAAABcAQAAAAAAAIDwAAAMAgAAAAAAAAAIAA0AGgAjAHsAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAAEqw==},
	bdsk-url-1 = {https://arxiv.org/pdf/1703.03400.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1703.03400}}

@article{Corazza2022,
	author = {Jan Corazza and Ivan Gavran and Daniel Neider},
	date-added = {2024-12-25 13:08:11 +0700},
	date-modified = {2024-12-25 13:09:45 +0700},
	title = {Reinforcement Learning with Stochastic Reward Machines},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEouLi9TdHVkeS9QYXBlcnMvUmVpbmZvcmNlbWVudCBMZWFybmluZyB3aXRoIFN0b2NoYXN0aWMgUmV3YXJkIE1hY2hpbmVzLnBkZk8RBBRib29rFAQAAAAABBAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAwAABQAAAAEBAABVc2VycwAAAAYAAAABAQAAc2NhbGVzAAAFAAAAAQEAAFN0dWR5AAAABgAAAAEBAABQYXBlcnMAADoAAAABAQAAUmVpbmZvcmNlbWVudCBMZWFybmluZyB3aXRoIFN0b2NoYXN0aWMgUmV3YXJkIE1hY2hpbmVzLnBkZgAAFAAAAAEGAAAEAAAAFAAAACQAAAA0AAAARAAAAAgAAAAEAwAApUEAAAAAAAAIAAAABAMAACnaBQAAAAAACAAAAAQDAAArmCQAAAAAAAgAAAAEAwAAc8dfAAAAAAAIAAAABAMAAO/DJgIAAAAAFAAAAAEGAACkAAAAtAAAAMQAAADUAAAA5AAAAAgAAAAABAAAQcaN7IWhfx0YAAAAAQIAAAEAAAAAAAAADwAAAAAAAAAAAAAAAAAAAAgAAAAEAwAAAwAAAAAAAAAEAAAAAwMAAPcBAAAIAAAAAQkAAGZpbGU6Ly8vDAAAAAEBAABNYWNpbnRvc2ggSEQIAAAABAMAAACQgpbnAAAACAAAAAAEAABBxrzVhQAAACQAAAABAQAANDM4NzY1ODQtNEI2OC00RUEzLUIwMDItRTQ0QTRCOTJBRDRDGAAAAAECAACBAAAAAQAAAO8TAAABAAAAAAAAAAAAAAABAAAAAQEAAC8AAAAAAAAAAQUAAAcBAAABAgAANzBhMmI0NGY0ZTNmMDgyY2E3ZDRiMjUwOTYzOTc5MDBiYWNhNjEyZmExMDhjODExODRiYWFiYmFmNThlMGU5MTswMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDAwMDAwMDIwO2NvbS5hcHBsZS5hcHAtc2FuZGJveC5yZWFkLXdyaXRlOzAxOzAxMDAwMDBmOzAwMDAwMDAwMDIyNmMzZWY7MDE7L3VzZXJzL3NjYWxlcy9zdHVkeS9wYXBlcnMvcmVpbmZvcmNlbWVudCBsZWFybmluZyB3aXRoIHN0b2NoYXN0aWMgcmV3YXJkIG1hY2hpbmVzLnBkZgAAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAiAAAAAAAAAAFEAAA9AAAAAAAAAAQEAAAIAEAAAAAAABAEAAAEAEAAAAAAAACIAAA7AEAAAAAAAAFIAAAXAEAAAAAAAAQIAAAbAEAAAAAAAARIAAAoAEAAAAAAAASIAAAgAEAAAAAAAATIAAAkAEAAAAAAAAgIAAAzAEAAAAAAAAwIAAA+AEAAAAAAAABwAAAQAEAAAAAAAARwAAAFAAAAAAAAAASwAAAUAEAAAAAAACA8AAAAAIAAAAAAAAACAANABoAIwBwAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABIg=}}

@article{Blauth2024,
	abstract = {In this paper, we present the Fast Optimizer Benchmark (FOB), a tool designed for evaluating deep learning optimizers during their development. The benchmark supports tasks from multiple domains such as computer vision, natural language processing, and graph learning. The focus is on convenient usage, featuring human-readable YAML configurations, SLURM integration, and plotting utilities. FOB can be used together with existing hyperparameter optimization (HPO) tools as it handles training and resuming of runs. The modular design enables integration into custom pipelines, using it simply as a collection of tasks. We showcase an optimizer comparison as a usage example of our tool. FOB can be found on GitHub: this https URL.},
	author = {Simon Blauth and Tobias B{\"u}rger and Zacharias H{\"a}ringer and J{\"o}rg Franke and Frank Hutter},
	bdsk-color = {9895935},
	date-added = {2024-12-25 11:01:56 +0700},
	date-modified = {2024-12-25 11:06:46 +0700},
	eprint = {2406.18701},
	keywords = {benchmark, deep learning, neural networks, training algorithms, optimization, optimizer,},
	month = {06},
	title = {Fast Optimizer Benchmark},
	url = {https://arxiv.org/pdf/2406.18701.pdf},
	year = {2024},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfECwuLi9TdHVkeS9QYXBlcnMvRmFzdCBPcHRpbWl6ZXIgQmVuY2htYXJrLnBkZk8RA9hib29r2AMAAAAABBAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADUAgAABQAAAAEBAABVc2VycwAAAAYAAAABAQAAc2NhbGVzAAAFAAAAAQEAAFN0dWR5AAAABgAAAAEBAABQYXBlcnMAABwAAAABAQAARmFzdCBPcHRpbWl6ZXIgQmVuY2htYXJrLnBkZhQAAAABBgAABAAAABQAAAAkAAAANAAAAEQAAAAIAAAABAMAAKVBAAAAAAAACAAAAAQDAAAp2gUAAAAAAAgAAAAEAwAAK5gkAAAAAAAIAAAABAMAAHPHXwAAAAAACAAAAAQDAADIvSYCAAAAABQAAAABBgAAhAAAAJQAAACkAAAAtAAAAMQAAAAIAAAAAAQAAEHGjd2vpMXVGAAAAAECAAABAAAAAAAAAA8AAAAAAAAAAAAAAAAAAAAIAAAABAMAAAMAAAAAAAAABAAAAAMDAAD3AQAACAAAAAEJAABmaWxlOi8vLwwAAAABAQAATWFjaW50b3NoIEhECAAAAAQDAAAAkIKW5wAAAAgAAAAABAAAQca81YUAAAAkAAAAAQEAADQzODc2NTg0LTRCNjgtNEVBMy1CMDAyLUU0NEE0QjkyQUQ0QxgAAAABAgAAgQAAAAEAAADvEwAAAQAAAAAAAAAAAAAAAQAAAAEBAAAvAAAAAAAAAAEFAADpAAAAAQIAADhhOTk3NjNhNjFmOGIwZTljYjE3ZjIyY2ExYjBlYzhlZTNkOTI1YWI0MDFkY2E5MzExMzg5ZWE0ZTdmYjk5NzA7MDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDAwMDAwMDAyMDtjb20uYXBwbGUuYXBwLXNhbmRib3gucmVhZC13cml0ZTswMTswMTAwMDAwZjswMDAwMDAwMDAyMjZiZGM4OzAxOy91c2Vycy9zY2FsZXMvc3R1ZHkvcGFwZXJzL2Zhc3Qgb3B0aW1pemVyIGJlbmNobWFyay5wZGYAAAAAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAaAAAAAAAAAAFEAAA1AAAAAAAAAAQEAAAAAEAAAAAAABAEAAA8AAAAAAAAAACIAAAzAEAAAAAAAAFIAAAPAEAAAAAAAAQIAAATAEAAAAAAAARIAAAgAEAAAAAAAASIAAAYAEAAAAAAAATIAAAcAEAAAAAAAAgIAAArAEAAAAAAAAwIAAA2AEAAAAAAAABwAAAIAEAAAAAAAARwAAAFAAAAAAAAAASwAAAMAEAAAAAAACA8AAA4AEAAAAAAAAACAANABoAIwBSAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABC4=},
	bdsk-url-1 = {https://arxiv.org/pdf/2406.18701.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/2406.18701}}

@article{Shmuel24,
	abstract = {The analysis of tabular datasets is highly prevalent both in scientific research and real-world applications of Machine Learning (ML). Unlike many other ML tasks, Deep Learning (DL) models often do not outperform traditional methods in this area. Previous comparative benchmarks have shown that DL performance is frequently equivalent or even inferior to models such as Gradient Boosting Machines (GBMs). In this study, we introduce a comprehensive benchmark aimed at better characterizing the types of datasets where DL models excel. Although several important benchmarks for tabular datasets already exist, our contribution lies in the variety and depth of our comparison: we evaluate 111 datasets with 20 different models, including both regression and classification tasks. These datasets vary in scale and include both those with and without categorical variables. Importantly, our benchmark contains a sufficient number of datasets where DL models perform best, allowing for a thorough analysis of the conditions under which DL models excel. Building on the results of this benchmark, we train a model that predicts scenarios where DL models outperform alternative methods with 86.1% accuracy (AUC 0.78). We present insights derived from this characterization and compare these findings to previous benchmarks.},
	author = {Assaf Shmuel and Oren Glickman and Teddy Lazebnik},
	date-added = {2024-12-23 15:43:58 +0700},
	date-modified = {2024-12-23 15:44:07 +0700},
	eprint = {2408.14817},
	month = {08},
	title = {A Comprehensive Benchmark of Machine and Deep Learning Across Diverse Tabular Datasets},
	url = {https://arxiv.org/pdf/2408.14817.pdf},
	year = {2024},
	bdsk-url-1 = {https://arxiv.org/pdf/2408.14817.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/2408.14817}}

@article{Dahl23,
	abstract = {Training algorithms, broadly construed, are an essential part of every deep learning pipeline. Training algorithm improvements that speed up training across a wide variety of workloads (e.g., better update rules, tuning protocols, learning rate schedules, or data selection schemes) could save time, save computational resources, and lead to better, more accurate, models. Unfortunately, as a community, we are currently unable to reliably identify training algorithm improvements, or even determine the state-of-the-art training algorithm. In this work, using concrete experiments, we argue that real progress in speeding up training requires new benchmarks that resolve three basic challenges faced by empirical comparisons of training algorithms: (1) how to decide when training is complete and precisely measure training time, (2) how to handle the sensitivity of measurements to exact workload details, and (3) how to fairly compare algorithms that require hyperparameter tuning. In order to address these challenges, we introduce a new, competitive, time-to-result benchmark using multiple workloads running on fixed hardware, the AlgoPerf: Training Algorithms benchmark. Our benchmark includes a set of workload variants that make it possible to detect benchmark submissions that are more robust to workload changes than current widely-used methods. Finally, we evaluate baseline submissions constructed using various optimizers that represent current practice, as well as other optimizers that have recently received attention in the literature. These baseline results collectively demonstrate the feasibility of our benchmark, show that non-trivial gaps between methods exist, and set a provisional state-of-the-art for future benchmark submissions to try and surpass.},
	author = {George E. Dahl and Frank Schneider and Zachary Nado and Naman Agarwal and Chandramouli Shama Sastry and Philipp Hennig and Sourabh Medapati and Runa Eschenhagen and Priya Kasimbeg and Daniel Suo and Juhan Bae and Justin Gilmer and Abel L. Peirson and Bilal Khan and Rohan Anil and Mike Rabbat and Shankar Krishnan and Daniel Snider and Ehsan Amid and Kongtao Chen and Chris J. Maddison and Rakshith Vasudev and Michal Badura and Ankush Garg and Peter Mattson},
	bdsk-color = {4},
	date-added = {2024-12-23 12:41:31 +0700},
	date-modified = {2024-12-25 10:28:41 +0700},
	eprint = {2306.07179},
	month = {06},
	title = {Benchmarking Neural Network Training Algorithms},
	url = {https://arxiv.org/pdf/2306.07179.pdf},
	year = {2023},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEMuLi9TdHVkeS9QYXBlcnMvQmVuY2htYXJraW5nIE5ldXJhbCBOZXR3b3JrIFRyYWluaW5nIEFsZ29yaXRobXMucGRmTxEEBGJvb2sEBAAAAAAEEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAAAFAAAAAQEAAFVzZXJzAAAABgAAAAEBAABzY2FsZXMAAAUAAAABAQAAU3R1ZHkAAAAGAAAAAQEAAFBhcGVycwAAMwAAAAEBAABCZW5jaG1hcmtpbmcgTmV1cmFsIE5ldHdvcmsgVHJhaW5pbmcgQWxnb3JpdGhtcy5wZGYAFAAAAAEGAAAEAAAAFAAAACQAAAA0AAAARAAAAAgAAAAEAwAApUEAAAAAAAAIAAAABAMAACnaBQAAAAAACAAAAAQDAAArmCQAAAAAAAgAAAAEAwAAc8dfAAAAAAAIAAAABAMAACKVJAIAAAAAFAAAAAEGAACcAAAArAAAALwAAADMAAAA3AAAAAgAAAAABAAAQcaMl9SOAfwYAAAAAQIAAAEAAAAAAAAADwAAAAAAAAAAAAAAAAAAAAgAAAAEAwAAAwAAAAAAAAAEAAAAAwMAAPcBAAAIAAAAAQkAAGZpbGU6Ly8vDAAAAAEBAABNYWNpbnRvc2ggSEQIAAAABAMAAACQgpbnAAAACAAAAAAEAABBxrzVhQAAACQAAAABAQAANDM4NzY1ODQtNEI2OC00RUEzLUIwMDItRTQ0QTRCOTJBRDRDGAAAAAECAACBAAAAAQAAAO8TAAABAAAAAAAAAAAAAAABAAAAAQEAAC8AAAAAAAAAAQUAAAABAAABAgAAN2IxMjY3ODBmNTM0ZjJmYWQ1NjM4ODk1NGQ0MDdjODI2YWMzOThkZTg0MjhhNDE4ZWVlZmFiMDczOGIxNDk2NzswMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDAwMDAwMDIwO2NvbS5hcHBsZS5hcHAtc2FuZGJveC5yZWFkLXdyaXRlOzAxOzAxMDAwMDBmOzAwMDAwMDAwMDIyNDk1MjI7MDE7L3VzZXJzL3NjYWxlcy9zdHVkeS9wYXBlcnMvYmVuY2htYXJraW5nIG5ldXJhbCBuZXR3b3JrIHRyYWluaW5nIGFsZ29yaXRobXMucGRmAMwAAAD+////AQAAAAAAAAAQAAAABBAAAIAAAAAAAAAABRAAAOwAAAAAAAAAEBAAABgBAAAAAAAAQBAAAAgBAAAAAAAAAiAAAOQBAAAAAAAABSAAAFQBAAAAAAAAECAAAGQBAAAAAAAAESAAAJgBAAAAAAAAEiAAAHgBAAAAAAAAEyAAAIgBAAAAAAAAICAAAMQBAAAAAAAAMCAAAPABAAAAAAAAAcAAADgBAAAAAAAAEcAAABQAAAAAAAAAEsAAAEgBAAAAAAAAgPAAAPgBAAAAAAAAAAgADQAaACMAaQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAARx},
	bdsk-url-1 = {https://arxiv.org/pdf/2306.07179.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/2306.07179}}

@article{Kamabattula21,
	abstract = {Training Deep neural networks (DNNs) on noisy labeled datasets is a challenging problem, because learning on mislabeled examples deteriorates the performance of the network. As the ground truth availability is limited with real-world noisy datasets, previous papers created synthetic noisy datasets by randomly modifying the labels of training examples of clean datasets. However, no final conclusions can be derived by just using this random noise, since it excludes feature-dependent noise. Thus, it is imperative to generate feature-dependent noisy datasets that additionally provide ground truth. Therefore, we propose an intuitive approach to creating feature-dependent noisy datasets by utilizing the training predictions of DNNs on clean datasets that also retain true label information. We refer to these datasets as "Pseudo Noisy datasets". We conduct several experiments to establish that Pseudo noisy datasets resemble feature-dependent noisy datasets across different conditions. We further randomly generate synthetic noisy datasets with the same noise distribution as that of Pseudo noise (referred as "Randomized Noise") to empirically show that i) learning is easier with feature-dependent label noise compared to random noise, ii) irrespective of noise distribution, Pseudo noisy datasets mimic feature-dependent label noise and iii) current training methods are not generalizable to feature-dependent label noise. Therefore, we believe that Pseudo noisy datasets will be quite helpful to study and develop robust training methods.},
	author = {Sree Ram Kamabattula and Kumudha Musini and Babak Namazi and Ganesh Sankaranarayanan and Venkat Devarajan},
	date-added = {2024-12-20 16:01:59 +0700},
	date-modified = {2024-12-20 16:02:33 +0700},
	eprint = {2105.10796},
	month = {05},
	title = {Generation and Analysis of Feature-Dependent Pseudo Noise for Training Deep Neural Networks},
	url = {https://arxiv.org/pdf/2105.10796.pdf},
	year = {2021},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEG8uLi9TdHVkeS9QYXBlcnMvR2VuZXJhdGlvbiBhbmQgQW5hbHlzaXMgb2YgRmVhdHVyZS1EZXBlbmRlbnQgUHNldWRvIE5vaXNlIGZvciBUcmFpbmluZyBEZWVwIE5ldXJhbCBOZXR3b3Jrcy5wZGZPEQRcYm9va1wEAAAAAAQQMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAWAMAAAUAAAABAQAAVXNlcnMAAAAGAAAAAQEAAHNjYWxlcwAABQAAAAEBAABTdHVkeQAAAAYAAAABAQAAUGFwZXJzAABfAAAAAQEAAEdlbmVyYXRpb24gYW5kIEFuYWx5c2lzIG9mIEZlYXR1cmUtRGVwZW5kZW50IFBzZXVkbyBOb2lzZSBmb3IgVHJhaW5pbmcgRGVlcCBOZXVyYWwgTmV0d29ya3MucGRmABQAAAABBgAABAAAABQAAAAkAAAANAAAAEQAAAAIAAAABAMAAKVBAAAAAAAACAAAAAQDAAAp2gUAAAAAAAgAAAAEAwAAK5gkAAAAAAAIAAAABAMAAHPHXwAAAAAACAAAAAQDAACkchoCAAAAABQAAAABBgAAyAAAANgAAADoAAAA+AAAAAgBAAAIAAAAAAQAAEHGirUbnaoaGAAAAAECAAABAAAAAAAAAA8AAAAAAAAAAAAAAAAAAAAIAAAABAMAAAMAAAAAAAAABAAAAAMDAAD3AQAACAAAAAEJAABmaWxlOi8vLwwAAAABAQAATWFjaW50b3NoIEhECAAAAAQDAAAAkIKW5wAAAAgAAAAABAAAQca81YUAAAAkAAAAAQEAADQzODc2NTg0LTRCNjgtNEVBMy1CMDAyLUU0NEE0QjkyQUQ0QxgAAAABAgAAgQAAAAEAAADvEwAAAQAAAAAAAAAAAAAAAQAAAAEBAAAvAAAAAAAAAAEFAAAsAQAAAQIAAGU4NGE2ZDM3YWNkYTlhM2Y5NTkzMDdjOTJjZGY0ZjE2NWU2YjlmNGRiZGMxOGFiZjY5MmQxYjM0NDUzNmYyNzU7MDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDAwMDAwMDAyMDtjb20uYXBwbGUuYXBwLXNhbmRib3gucmVhZC13cml0ZTswMTswMTAwMDAwZjswMDAwMDAwMDAyMWE3MmE0OzAxOy91c2Vycy9zY2FsZXMvc3R1ZHkvcGFwZXJzL2dlbmVyYXRpb24gYW5kIGFuYWx5c2lzIG9mIGZlYXR1cmUtZGVwZW5kZW50IHBzZXVkbyBub2lzZSBmb3IgdHJhaW5pbmcgZGVlcCBuZXVyYWwgbmV0d29ya3MucGRmAMwAAAD+////AQAAAAAAAAAQAAAABBAAAKwAAAAAAAAABRAAABgBAAAAAAAAEBAAAEQBAAAAAAAAQBAAADQBAAAAAAAAAiAAABACAAAAAAAABSAAAIABAAAAAAAAECAAAJABAAAAAAAAESAAAMQBAAAAAAAAEiAAAKQBAAAAAAAAEyAAALQBAAAAAAAAICAAAPABAAAAAAAAMCAAABwCAAAAAAAAAcAAAGQBAAAAAAAAEcAAABQAAAAAAAAAEsAAAHQBAAAAAAAAgPAAACQCAAAAAAAAAAgADQAaACMAlQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAT1},
	bdsk-url-1 = {https://arxiv.org/pdf/2105.10796.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/2105.10796}}

@article{Lewis:2016aa,
	abstract = {Rewards are commonly used in interventions to change behavior, but they can inhibit development of intrinsic motivation, which is associated with long-term behavior maintenance. Gamification is a novel intervention strategy that may target intrinsic motivation through fun and enjoyment. Before the effects of gamified interventions on motivation can be determined, there must be an understanding of how gamified interventions operationalize rewards, such as point systems. The purpose of this review is to determine the prevalence of different reward types, specifically point systems, within gamified interventions. Electronic databases were searched for relevant articles. Data sources included Medline OVID, Medline PubMed, Web of Science, CINAHL, Cochrane Central, and PsycINFO. Out of the 21 articles retrieved, 18 studies described a reward system and were included in this review. Gamified interventions were designed to target a myriad of clinical outcomes across diverse populations. Rewards included points (n =14), achievements/badges/medals (n = 7), tangible rewards (n = 7), currency (n = 4), other unspecified rewards (n = 3), likes (n = 2), animated feedback (n = 1), and kudos (n = 1). Rewards, and points in particular, appear to be a foundational component of gamified interventions. Despite their prevalence, authors seldom described the use of noncontingent rewards or how the rewards interacted with other game features. The reward systems relying on tangible rewards and currency may have been limited by inhibited intrinsic motivation. As gamification proliferates, future research should explicitly describe how rewards were operationalized in the intervention and evaluate the effects of gamified rewards on motivation across populations and research outcomes.},
	author = {Lewis, Zakkoyya H and Swartz, Maria C and Lyons, Elizabeth J},
	date-added = {2024-12-19 14:18:22 +0700},
	date-modified = {2024-12-19 14:23:10 +0700},
	journal = {Games for health journal},
	number = {2},
	pages = {93--99},
	publisher = {Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA},
	title = {What's the point?: a review of reward systems implemented in gamification interventions},
	volume = {5},
	year = {2016}}

@article{icard21,
	abstract = {When does it make sense to act randomly? A persuasive argument from Bayesian decision theory legitimizes randomization essentially only in tie-breaking situations. Rational behaviour in humans, non-human animals, and artificial agents, however, often seems indeterminate, even random. Moreover, rationales for randomized acts have been offered in a number of disciplines, including game theory, experimental design, and machine learning. A common way of accommodating some of these observations is by appeal to a decision-maker's bounded computational resources. Making this suggestion both precise and compelling is surprisingly difficult. Toward this end, I propose two fundamental rationales for randomization, drawing upon diverse ideas and results from the wider theory of computation. The first unifies common intuitions in favour of randomization from the aforementioned disciplines. The second introduces a deep connection between randomization and memory: access to a randomizing device is provably helpful for an agent burdened with a finite memory. Aside from fit with ordinary intuitions about rational action, the two rationales also make sense of empirical observations in the biological world. Indeed, random behaviour emerges more or less where it should, according to the proposal.},
	author = {Thomas Icard},
	bdsk-color = {7},
	citation_author = {Icard, Thomas},
	citation_author_institution = {Stanford University},
	citation_issn = {0026-4423},
	citation_pdf_url = {https://academic.oup.com/mind/article-pdf/130/517/111/38461032/fzz065.pdf},
	citation_publication_date = {2021/06/01},
	citation_publisher = {Oxford Academic},
	citation_reference = {citation_title=Why There's No Cause to Randomize'; citation_author=Worrall John; citation_journal_title=British Journal for the Philosophy of Science; citation_year=2007; citation_volume=58; citation_issue=3; citation_pages=451-88;},
	citation_title = {Why Be Random?},
	citation_xml_url = {https://academic.oup.com/mind/article-xml/130/517/111/5611345},
	date-added = {2024-12-19 14:13:44 +0700},
	date-modified = {2024-12-19 14:14:56 +0700},
	description = {Abstract. When does it make sense to act randomly? A persuasive argument from Bayesian decision theory legitimizes randomization essentially only in tie-br},
	doi = {https://doi.org/10.1093/mind/fzz065},
	format-detection = {telephone=no},
	journal = {Mind},
	month = {January},
	msapplication-config = {//oup.silverchair-cdn.com/UI/app/img/v-638689475255913527/browserconfig.xml},
	number = {517},
	pages = {111-139},
	product_code = {I_130190},
	theme-color = {#002f65},
	title = {Why Be Random?},
	twitter:card = {summary_large_image},
	viewport = {width=device-width, initial-scale=1, maximum-scale=10},
	volume = {130},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1093/mind/fzz065}}

@inproceedings{tang2021discovering,
	author = {Zhenggang Tang and Chao Yu and Boyuan Chen and Huazhe Xu and Xiaolong Wang and Fei Fang and Simon Shaolei Du and Yu Wang and Yi Wu},
	bdsk-color = {6},
	booktitle = {International Conference on Learning Representations},
	date-modified = {2024-12-19 13:35:47 +0700},
	keywords = {reward randomization, multi-agent games, trust dilemmas},
	title = {Discovering Diverse Multi-Agent Strategic Behavior via Reward Randomization},
	url = {https://openreview.net/forum?id=lvRTC669EY_},
	year = {2021},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEF8uLi9TdHVkeS9QYXBlcnMvRGlzY292ZXJpbmcgRGl2ZXJzZSBNdWx0aS1BZ2VudCBTdHJhdGVnaWMgQmVoYXZpb3IgdmlhIFJld2FyZCBSYW5kb21pemF0aW9uLnBkZk8RBDxib29rPAQAAAAABBAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4AwAABQAAAAEBAABVc2VycwAAAAYAAAABAQAAc2NhbGVzAAAFAAAAAQEAAFN0dWR5AAAABgAAAAEBAABQYXBlcnMAAE8AAAABAQAARGlzY292ZXJpbmcgRGl2ZXJzZSBNdWx0aS1BZ2VudCBTdHJhdGVnaWMgQmVoYXZpb3IgdmlhIFJld2FyZCBSYW5kb21pemF0aW9uLnBkZgAUAAAAAQYAAAQAAAAUAAAAJAAAADQAAABEAAAACAAAAAQDAAClQQAAAAAAAAgAAAAEAwAAKdoFAAAAAAAIAAAABAMAACuYJAAAAAAACAAAAAQDAABzx18AAAAAAAgAAAAEAwAAoD4ZAgAAAAAUAAAAAQYAALgAAADIAAAA2AAAAOgAAAD4AAAACAAAAAAEAABBxon6aFQF3xgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAADAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAJCClucAAAAIAAAAAAQAAEHGvNWFAAAAJAAAAAEBAAA0Mzg3NjU4NC00QjY4LTRFQTMtQjAwMi1FNDRBNEI5MkFENEMYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAAHAEAAAECAAA2NDM4YzE1NjljMjVkNjEyM2YxN2U1OGYwNGMxZDQwNDQ4NWRkYjAzMDc2ZjJhYjliMjQ1YzVhNDIyNDgxNTEwOzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMGY7MDAwMDAwMDAwMjE5M2VhMDswMTsvdXNlcnMvc2NhbGVzL3N0dWR5L3BhcGVycy9kaXNjb3ZlcmluZyBkaXZlcnNlIG11bHRpLWFnZW50IHN0cmF0ZWdpYyBiZWhhdmlvciB2aWEgcmV3YXJkIHJhbmRvbWl6YXRpb24ucGRmAMwAAAD+////AQAAAAAAAAAQAAAABBAAAJwAAAAAAAAABRAAAAgBAAAAAAAAEBAAADQBAAAAAAAAQBAAACQBAAAAAAAAAiAAAAACAAAAAAAABSAAAHABAAAAAAAAECAAAIABAAAAAAAAESAAALQBAAAAAAAAEiAAAJQBAAAAAAAAEyAAAKQBAAAAAAAAICAAAOABAAAAAAAAMCAAAAwCAAAAAAAAAcAAAFQBAAAAAAAAEcAAABQAAAAAAAAAEsAAAGQBAAAAAAAAgPAAABQCAAAAAAAAAAgADQAaACMAhQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAATF},
	bdsk-url-1 = {https://openreview.net/forum?id=lvRTC669EY_}}

@article{AmosKolter2017,
	abstract = {This paper presents OptNet, a network architecture that integrates optimization problems (here, specifically in the form of quadratic programs) as individual layers in larger end-to-end trainable deep networks. These layers encode constraints and complex dependencies between the hidden states that traditional convolutional and fully-connected layers often cannot capture. We explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast GPU-based batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems. In one notable example, the method is learns to play mini-Sudoku (4x4) given just input and output games, with no a-priori information about the rules of the game; this highlights the ability of OptNet to learn hard constraints better than other neural architectures.},
	author = {Brandon Amos and J. Zico Kolter},
	bdsk-color = {1},
	date-added = {2024-12-17 23:18:06 +0700},
	date-modified = {2024-12-17 23:18:29 +0700},
	eprint = {1703.00443},
	month = {03},
	title = {OptNet: Differentiable Optimization as a Layer in Neural Networks},
	url = {https://arxiv.org/pdf/1703.00443.pdf},
	year = {2017},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEFYuLi9TdHVkeS9QYXBlcnMvT3B0TmV0IC0gRGlmZmVyZW50aWFibGUgT3B0aW1pemF0aW9uIGFzIGEgTGF5ZXIgaW4gTmV1cmFsIE5ldHdvcmtzLnBkZk8RBCxib29rLAQAAAAABBAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAoAwAABQAAAAEBAABVc2VycwAAAAYAAAABAQAAc2NhbGVzAAAFAAAAAQEAAFN0dWR5AAAABgAAAAEBAABQYXBlcnMAAEYAAAABAQAAT3B0TmV0IC0gRGlmZmVyZW50aWFibGUgT3B0aW1pemF0aW9uIGFzIGEgTGF5ZXIgaW4gTmV1cmFsIE5ldHdvcmtzLnBkZgAAFAAAAAEGAAAEAAAAFAAAACQAAAA0AAAARAAAAAgAAAAEAwAApUEAAAAAAAAIAAAABAMAACnaBQAAAAAACAAAAAQDAAArmCQAAAAAAAgAAAAEAwAAc8dfAAAAAAAIAAAABAMAAB4CVwIAAAAAFAAAAAEGAACwAAAAwAAAANAAAADgAAAA8AAAAAgAAAAABAAAQcaeouaF2dUYAAAAAQIAAAEAAAAAAAAADwAAAAAAAAAAAAAAAAAAAAgAAAAEAwAAAwAAAAAAAAAEAAAAAwMAAPcBAAAIAAAAAQkAAGZpbGU6Ly8vDAAAAAEBAABNYWNpbnRvc2ggSEQIAAAABAMAAACQgpbnAAAACAAAAAAEAABBxrzVhQAAACQAAAABAQAANDM4NzY1ODQtNEI2OC00RUEzLUIwMDItRTQ0QTRCOTJBRDRDGAAAAAECAACBAAAAAQAAAO8TAAABAAAAAAAAAAAAAAABAAAAAQEAAC8AAAAAAAAAAQUAABMBAAABAgAAY2JhNjJkNzkwZjVjMzI2ZTU4MWJiM2E2NGE1NzY3MDE1ZWU5MTMzYzk2YTljNWQ4ZmU2Y2M0NjRlYmZiMzhjMzswMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDAwMDAwMDIwO2NvbS5hcHBsZS5hcHAtc2FuZGJveC5yZWFkLXdyaXRlOzAxOzAxMDAwMDBmOzAwMDAwMDAwMDI1NzAyMWU7MDE7L3VzZXJzL3NjYWxlcy9zdHVkeS9wYXBlcnMvb3B0bmV0IC0gZGlmZmVyZW50aWFibGUgb3B0aW1pemF0aW9uIGFzIGEgbGF5ZXIgaW4gbmV1cmFsIG5ldHdvcmtzLnBkZgAAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAlAAAAAAAAAAFEAAAAAEAAAAAAAAQEAAALAEAAAAAAABAEAAAHAEAAAAAAAACIAAA+AEAAAAAAAAFIAAAaAEAAAAAAAAQIAAAeAEAAAAAAAARIAAArAEAAAAAAAASIAAAjAEAAAAAAAATIAAAnAEAAAAAAAAgIAAA2AEAAAAAAAAwIAAABAIAAAAAAAABwAAATAEAAAAAAAARwAAAFAAAAAAAAAASwAAAXAEAAAAAAACA8AAADAIAAAAAAAAACAANABoAIwB8AAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABKw=},
	bdsk-url-1 = {https://arxiv.org/pdf/1703.00443.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1703.00443}}

@article{amoskotler17,
	date-added = {2024-12-17 23:14:11 +0700},
	date-modified = {2024-12-17 23:14:22 +0700}}

@article{Avignon88,
	author = {Marko Bohanec and Vladislav Rajkovi{\v c}},
	bdsk-color = {6},
	date-added = {2024-12-16 16:05:07 +0700},
	date-modified = {2024-12-16 16:18:11 +0700},
	journal = {Proceedings of the 8th International Workshop 'Expert Systems and Their Applications AVIGNON 88'},
	keywords = {decision making, expert systems, knowledge acquisition, multi-attribute decision making, knowledge representation, knowledge explanation},
	pages = {59-78},
	title = {Knowledge Acqusition and Explanation for Multi-Attribute Decision Making},
	volume = {1},
	year = {1988}}

@article{composing23,
	annote = {Game theory is used by all behavioral sciences, but its development has long centered around the economic interpretation of equilibrium outcomes in relatively simple games and toy systems. But game theory has another potential use: the high-level design of large game compositions that express complex architectures and represent real-world institutions faith- fully. Compositional game theory, grounded in the mathematics underlying programming languages, and introduced here as a general computational framework, increases the parsi- mony of game representations with abstraction and modularity, accelerates search and design, and helps theorists across disciplines express real-world institutional complexity in well-defined ways. Relative to existing approaches in game theory, compositional game the- ory is especially promising for solving game systems with long-range dependencies, for comparing large numbers of structurally related games, and for nesting games into the larger logical or strategic flows typical of real world policy or institutional systems.},
	author = {Seth Frey and Jules Hedges and Joshua Tan and Philipp Zahn},
	bdsk-color = {2},
	date-added = {2024-12-04 11:49:57 +0700},
	date-modified = {2024-12-04 11:54:25 +0700},
	journal = {PLoS ONE},
	keywords = {game theory, behavioral sciences, compositional game theory, computational framework,},
	number = {3},
	title = {Composing games into complex institutions},
	volume = {18},
	year = {2023},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfED0uLi9TdHVkeS9QYXBlcnMvQ29tcG9zaW5nIEdhbWVzIGludG8gQ29tcGxleCBJbnN0aXR1dGlvbnMucGRmTxED/GJvb2v8AwAAAAAEEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPgCAAAFAAAAAQEAAFVzZXJzAAAABgAAAAEBAABzY2FsZXMAAAUAAAABAQAAU3R1ZHkAAAAGAAAAAQEAAFBhcGVycwAALQAAAAEBAABDb21wb3NpbmcgR2FtZXMgaW50byBDb21wbGV4IEluc3RpdHV0aW9ucy5wZGYAAAAUAAAAAQYAAAQAAAAUAAAAJAAAADQAAABEAAAACAAAAAQDAAClQQAAAAAAAAgAAAAEAwAAKdoFAAAAAAAIAAAABAMAACuYJAAAAAAACAAAAAQDAABzx18AAAAAAAgAAAAEAwAApzX+AQAAAAAUAAAAAQYAAJgAAACoAAAAuAAAAMgAAADYAAAACAAAAAAEAABBxoALbCQRyRgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAADAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAJCClucAAAAIAAAAAAQAAEHGvNWFAAAAJAAAAAEBAAA0Mzg3NjU4NC00QjY4LTRFQTMtQjAwMi1FNDRBNEI5MkFENEMYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAA+gAAAAECAAAzYmIxNGFkMzI2MzRkMTJkODVkMDAzYjU4YTJjZmMyMjYyYjdiMTYxMDQzYjYxNTVmYWJiNDRiMDdiNWE1MjI5OzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMGY7MDAwMDAwMDAwMWZlMzVhNzswMTsvdXNlcnMvc2NhbGVzL3N0dWR5L3BhcGVycy9jb21wb3NpbmcgZ2FtZXMgaW50byBjb21wbGV4IGluc3RpdHV0aW9ucy5wZGYAAADMAAAA/v///wEAAAAAAAAAEAAAAAQQAAB8AAAAAAAAAAUQAADoAAAAAAAAABAQAAAUAQAAAAAAAEAQAAAEAQAAAAAAAAIgAADgAQAAAAAAAAUgAABQAQAAAAAAABAgAABgAQAAAAAAABEgAACUAQAAAAAAABIgAAB0AQAAAAAAABMgAACEAQAAAAAAACAgAADAAQAAAAAAADAgAADsAQAAAAAAAAHAAAA0AQAAAAAAABHAAAAUAAAAAAAAABLAAABEAQAAAAAAAIDwAAD0AQAAAAAAAAAIAA0AGgAjAGMAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAAEYw==}}

@inproceedings{zinkevich03,
	abstract = {Convex programming involves a convex set
F ⊆ R
n and a convex cost function c :
F → R. The goal of convex programming
is to find a point in F which minimizes c.
In online convex programming, the convex
set is known in advance, but in each step
of some repeated optimization problem, one
must select a point in F before seeing the cost
function for that step. This can be used to
model factory production, farm production,
and many other industrial optimization problems where one is unaware of the value of the
items produced until they have already been
constructed. We introduce an algorithm for
this domain. We also apply this algorithm
to repeated games, and show that it is really a generalization of infinitesimal gradient
ascent, and the results here imply that generalized infinitesimal gradient ascent (GIGA)
is universally consistent.},
	author = {Martin Zinkevich},
	bdsk-color = {2},
	booktitle = {Proceedings of the Twentieth International Conference on Machine Learning (ICML-2003)},
	date-added = {2024-12-02 10:16:20 +0700},
	date-modified = {2024-12-02 10:19:11 +0700},
	keywords = {optimization, machine learning, online convex programming, linear programming, repeated games, gradient ascent},
	title = {Online Convex Programming and Generalized Infinitesimal Gradient Ascent},
	year = {2003},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEFsuLi9TdHVkeS9QYXBlcnMvT25saW5lIENvbnZleCBQcm9ncmFtbWluZyBhbmQgR2VuZXJhbGl6ZWQgSW5maW5pdGVzaW1hbCBHcmFkaWVudCBBc2NlbnQucGRmTxEENGJvb2s0BAAAAAAEEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADADAAAFAAAAAQEAAFVzZXJzAAAABgAAAAEBAABzY2FsZXMAAAUAAAABAQAAU3R1ZHkAAAAGAAAAAQEAAFBhcGVycwAASwAAAAEBAABPbmxpbmUgQ29udmV4IFByb2dyYW1taW5nIGFuZCBHZW5lcmFsaXplZCBJbmZpbml0ZXNpbWFsIEdyYWRpZW50IEFzY2VudC5wZGYAFAAAAAEGAAAEAAAAFAAAACQAAAA0AAAARAAAAAgAAAAEAwAApUEAAAAAAAAIAAAABAMAACnaBQAAAAAACAAAAAQDAAArmCQAAAAAAAgAAAAEAwAAc8dfAAAAAAAIAAAABAMAADey9QEAAAAAFAAAAAEGAAC0AAAAxAAAANQAAADkAAAA9AAAAAgAAAAABAAAQcZ+rvaOc5YYAAAAAQIAAAEAAAAAAAAADwAAAAAAAAAAAAAAAAAAAAgAAAAEAwAAAwAAAAAAAAAEAAAAAwMAAPcBAAAIAAAAAQkAAGZpbGU6Ly8vDAAAAAEBAABNYWNpbnRvc2ggSEQIAAAABAMAAACQgpbnAAAACAAAAAAEAABBxrzVhQAAACQAAAABAQAANDM4NzY1ODQtNEI2OC00RUEzLUIwMDItRTQ0QTRCOTJBRDRDGAAAAAECAACBAAAAAQAAAO8TAAABAAAAAAAAAAAAAAABAAAAAQEAAC8AAAAAAAAAAQUAABgBAAABAgAANTI3YzA4MzVhZmY5YzZjOTc0ZGMxN2VlMmU4NGFiMDRjZTdiYTcwM2Y0NDFiZmU0ZWUxYTAxNDBkNDRkMjFhMDswMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDAwMDAwMDIwO2NvbS5hcHBsZS5hcHAtc2FuZGJveC5yZWFkLXdyaXRlOzAxOzAxMDAwMDBmOzAwMDAwMDAwMDFmNWIyMzc7MDE7L3VzZXJzL3NjYWxlcy9zdHVkeS9wYXBlcnMvb25saW5lIGNvbnZleCBwcm9ncmFtbWluZyBhbmQgZ2VuZXJhbGl6ZWQgaW5maW5pdGVzaW1hbCBncmFkaWVudCBhc2NlbnQucGRmAMwAAAD+////AQAAAAAAAAAQAAAABBAAAJgAAAAAAAAABRAAAAQBAAAAAAAAEBAAADABAAAAAAAAQBAAACABAAAAAAAAAiAAAPwBAAAAAAAABSAAAGwBAAAAAAAAECAAAHwBAAAAAAAAESAAALABAAAAAAAAEiAAAJABAAAAAAAAEyAAAKABAAAAAAAAICAAANwBAAAAAAAAMCAAAAgCAAAAAAAAAcAAAFABAAAAAAAAEcAAABQAAAAAAAAAEsAAAGABAAAAAAAAgPAAABACAAAAAAAAAAgADQAaACMAgQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAS5}}

@book{qin17,
	abstract = {Big Model analytics tackles the training of massive models that go beyond the available memory of a single
computing device, e.g., CPU or GPU. It generalizes Big Data analytics which is targeted at how to train memoryresident models over out-of-memory training data. In this paper, we propose an in-database solution for Big Model
analytics. We identify dot-product as the primary operation for training generalized linear models and introduce
the first array-relation dot-product join database operator between a set of sparse arrays and a dense relation. This
is a constrained formulation of the extensively studied sparse matrix vector multiplication (SpMV) kernel. The
paramount challenge in designing the dot-product join operator is how to optimally schedule access to the dense
relation based on the non-contiguous entries in the sparse arrays. We prove that this problem is NP-hard and propose
a practical solution characterized by two technical contributions---dynamic batch processing and array reordering.
We devise three heuristics -- LSH, Radix, and K-center -- for array reordering and analyze them thoroughly. We
execute extensive experiments over synthetic and real data that confirm the minimal overhead the operator incurs
when sufficient memory is available and the graceful degradation it suffers as memory becomes scarce. Moreover,
dot-product join achieves an order of magnitude reduction in execution time over alternative in-database solutions.},
	author = {Chengjie Qin and Florin Rusu},
	date-added = {2024-12-02 10:00:52 +0700},
	date-modified = {2024-12-02 10:03:43 +0700},
	keywords = {big data, big model, databases, sparse matrix vector multiplication, dot-product, batch processing, array reordering},
	month = {January},
	title = {Dot-Product Join: An Array-Relation Join Operator for Big Model Analytics},
	year = {2017},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEF0uLi9TdHVkeS9QYXBlcnMvRG90LVByb2R1Y3QgSm9pbi0gQW4gQXJyYXktUmVsYXRpb24gSm9pbiBPcGVyYXRvciBmb3IgQmlnIE1vZGVsIEFuYWx5dGljcy5wZGZPEQQ8Ym9vazwEAAAAAAQQMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOAMAAAUAAAABAQAAVXNlcnMAAAAGAAAAAQEAAHNjYWxlcwAABQAAAAEBAABTdHVkeQAAAAYAAAABAQAAUGFwZXJzAABNAAAAAQEAAERvdC1Qcm9kdWN0IEpvaW4tIEFuIEFycmF5LVJlbGF0aW9uIEpvaW4gT3BlcmF0b3IgZm9yIEJpZyBNb2RlbCBBbmFseXRpY3MucGRmAAAAFAAAAAEGAAAEAAAAFAAAACQAAAA0AAAARAAAAAgAAAAEAwAApUEAAAAAAAAIAAAABAMAACnaBQAAAAAACAAAAAQDAAArmCQAAAAAAAgAAAAEAwAAc8dfAAAAAAAIAAAABAMAACCm9QEAAAAAFAAAAAEGAAC4AAAAyAAAANgAAADoAAAA+AAAAAgAAAAABAAAQcZ+rR5I9k0YAAAAAQIAAAEAAAAAAAAADwAAAAAAAAAAAAAAAAAAAAgAAAAEAwAAAwAAAAAAAAAEAAAAAwMAAPcBAAAIAAAAAQkAAGZpbGU6Ly8vDAAAAAEBAABNYWNpbnRvc2ggSEQIAAAABAMAAACQgpbnAAAACAAAAAAEAABBxrzVhQAAACQAAAABAQAANDM4NzY1ODQtNEI2OC00RUEzLUIwMDItRTQ0QTRCOTJBRDRDGAAAAAECAACBAAAAAQAAAO8TAAABAAAAAAAAAAAAAAABAAAAAQEAAC8AAAAAAAAAAQUAABoBAAABAgAAMGU4MWI4MTNhMjkxYjA5OTA4MzE2NjBlZTQ3NzZkMWFiOGJiODZjMzAyZDFhNDk1YTg5YjdmMjM1ZmY5OGVmNzswMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDAwMDAwMDIwO2NvbS5hcHBsZS5hcHAtc2FuZGJveC5yZWFkLXdyaXRlOzAxOzAxMDAwMDBmOzAwMDAwMDAwMDFmNWE2MjA7MDE7L3VzZXJzL3NjYWxlcy9zdHVkeS9wYXBlcnMvZG90LXByb2R1Y3Qgam9pbi0gYW4gYXJyYXktcmVsYXRpb24gam9pbiBvcGVyYXRvciBmb3IgYmlnIG1vZGVsIGFuYWx5dGljcy5wZGYAAADMAAAA/v///wEAAAAAAAAAEAAAAAQQAACcAAAAAAAAAAUQAAAIAQAAAAAAABAQAAA0AQAAAAAAAEAQAAAkAQAAAAAAAAIgAAAAAgAAAAAAAAUgAABwAQAAAAAAABAgAACAAQAAAAAAABEgAAC0AQAAAAAAABIgAACUAQAAAAAAABMgAACkAQAAAAAAACAgAADgAQAAAAAAADAgAAAMAgAAAAAAAAHAAABUAQAAAAAAABHAAAAUAAAAAAAAABLAAABkAQAAAAAAAIDwAAAUAgAAAAAAAAAIAA0AGgAjAIMAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAAEww==}}

@article{Tolstikhin21,
	abstract = {Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. "mixing" the per-location features), and one with MLPs applied across patches (i.e. "mixing" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.},
	author = {Ilya Tolstikhin and Neil Houlsby and Alexander Kolesnikov and Lucas Beyer and Xiaohua Zhai and Thomas Unterthiner and Jessica Yung and Andreas Steiner and Daniel Keysers and Jakob Uszkoreit and Mario Lucic and Alexey Dosovitskiy},
	date-added = {2024-12-01 10:29:02 +0700},
	date-modified = {2024-12-01 10:30:20 +0700},
	eprint = {2105.01601},
	keywords = {attention, vision transformer, multi-layer perceptron, computer vision},
	month = {05},
	title = {MLP-Mixer: An all-MLP Architecture for Vision},
	url = {https://arxiv.org/pdf/2105.01601.pdf},
	year = {2021},
	bdsk-url-1 = {https://arxiv.org/pdf/2105.01601.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/2105.01601}}

@book{shoham10,
	author = {Yoav Shoham and Kevin Leyton-Brown},
	date-added = {2024-11-30 21:02:30 +0700},
	date-modified = {2024-11-30 21:04:27 +0700},
	title = {Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations},
	year = {2010},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEBcuLi9TdHVkeS9QYXBlcnMvbWFzLnBkZk8RA6xib29rrAMAAAAABBAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACoAgAABQAAAAEBAABVc2VycwAAAAYAAAABAQAAc2NhbGVzAAAFAAAAAQEAAFN0dWR5AAAABgAAAAEBAABQYXBlcnMAAAcAAAABAQAAbWFzLnBkZgAUAAAAAQYAAAQAAAAUAAAAJAAAADQAAABEAAAACAAAAAQDAAClQQAAAAAAAAgAAAAEAwAAKdoFAAAAAAAIAAAABAMAACuYJAAAAAAACAAAAAQDAABzx18AAAAAAAgAAAAEAwAA7pDyAQAAAAAUAAAAAQYAAHAAAACAAAAAkAAAAKAAAACwAAAACAAAAAAEAABBxn2pNFejsRgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAADAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAJCClucAAAAIAAAAAAQAAEHGvNWFAAAAJAAAAAEBAAA0Mzg3NjU4NC00QjY4LTRFQTMtQjAwMi1FNDRBNEI5MkFENEMYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAA1AAAAAECAAA1YmNmZDU0MTY0MDdlNTFhMGRiNTkxZjRhMDMxMjY0MTE1ZTIyNjdlNjY3OTNlM2M2ZDBhZjE4NzE3YTRiMWMzOzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMGY7MDAwMDAwMDAwMWYyOTBlZTswMTsvdXNlcnMvc2NhbGVzL3N0dWR5L3BhcGVycy9tYXMucGRmAMwAAAD+////AQAAAAAAAAAQAAAABBAAAFQAAAAAAAAABRAAAMAAAAAAAAAAEBAAAOwAAAAAAAAAQBAAANwAAAAAAAAAAiAAALgBAAAAAAAABSAAACgBAAAAAAAAECAAADgBAAAAAAAAESAAAGwBAAAAAAAAEiAAAEwBAAAAAAAAEyAAAFwBAAAAAAAAICAAAJgBAAAAAAAAMCAAAMQBAAAAAAAAAcAAAAwBAAAAAAAAEcAAABQAAAAAAAAAEsAAABwBAAAAAAAAgPAAAMwBAAAAAAAAAAgADQAaACMAPQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAPt}}

@article{hedges17,
	author = {Jules Hedges and Paulo Oliva and Evguenia Shprits and Viktor Winschel and Philipp Zahn},
	date-added = {2024-11-30 19:52:51 +0700},
	date-modified = {2024-11-30 19:55:47 +0700},
	editor = {Yuliya Lierler and Walid Taha},
	journal = {Lecture Notes in Computer Science},
	pages = {136-151},
	series = {Practical aspects of declarative languages},
	title = {Selection Equilibria of Higher-Order Games},
	volume = {10137},
	year = {2017}}

@article{pavlovic09,
	author = {Dusko Pavlovic},
	date-added = {2024-11-30 19:49:33 +0700},
	date-modified = {2024-11-30 19:50:14 +0700},
	title = {A Semantical Approach to Equilibria and Rationality}}

@article{Kingma2014,
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	author = {Diederik P. Kingma and Jimmy Ba},
	date-added = {2024-11-30 18:45:53 +0700},
	date-modified = {2024-12-01 10:31:16 +0700},
	eprint = {1412.6980},
	keywords = {stochastic gradient descent, lower-order moments, sparse gradients, optimization},
	month = {12},
	title = {Adam: A Method for Stochastic Optimization},
	url = {https://arxiv.org/pdf/1412.6980.pdf},
	year = {2014},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfED8uLi9TdHVkeS9QYXBlcnMvQWRhbSAtIGEgTWV0aG9kIGZvciBTdG9jaGFzdGljIE9wdGltaXphdGlvbi5wZGZPEQP8Ym9va/wDAAAAAAQQMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA+AIAAAUAAAABAQAAVXNlcnMAAAAGAAAAAQEAAHNjYWxlcwAABQAAAAEBAABTdHVkeQAAAAYAAAABAQAAUGFwZXJzAAAvAAAAAQEAAEFkYW0gLSBhIE1ldGhvZCBmb3IgU3RvY2hhc3RpYyBPcHRpbWl6YXRpb24ucGRmABQAAAABBgAABAAAABQAAAAkAAAANAAAAEQAAAAIAAAABAMAAKVBAAAAAAAACAAAAAQDAAAp2gUAAAAAAAgAAAAEAwAAK5gkAAAAAAAIAAAABAMAAHPHXwAAAAAACAAAAAQDAACPbfIBAAAAABQAAAABBgAAmAAAAKgAAAC4AAAAyAAAANgAAAAIAAAAAAQAAEHGfZlX3Bz+GAAAAAECAAABAAAAAAAAAA8AAAAAAAAAAAAAAAAAAAAIAAAABAMAAAMAAAAAAAAABAAAAAMDAAD3AQAACAAAAAEJAABmaWxlOi8vLwwAAAABAQAATWFjaW50b3NoIEhECAAAAAQDAAAAkIKW5wAAAAgAAAAABAAAQca81YUAAAAkAAAAAQEAADQzODc2NTg0LTRCNjgtNEVBMy1CMDAyLUU0NEE0QjkyQUQ0QxgAAAABAgAAgQAAAAEAAADvEwAAAQAAAAAAAAAAAAAAAQAAAAEBAAAvAAAAAAAAAAEFAAD8AAAAAQIAADIxNjA5ZmJjOWY0YTk1MTgwYmRmNTM1Y2M3ZmYwY2Y5YmNlMmNiMzhmODRkMDRlY2Q2MDFhYzE5YmQyYmRkODU7MDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDAwMDAwMDAyMDtjb20uYXBwbGUuYXBwLXNhbmRib3gucmVhZC13cml0ZTswMTswMTAwMDAwZjswMDAwMDAwMDAxZjI2ZDhmOzAxOy91c2Vycy9zY2FsZXMvc3R1ZHkvcGFwZXJzL2FkYW0gLSBhIG1ldGhvZCBmb3Igc3RvY2hhc3RpYyBvcHRpbWl6YXRpb24ucGRmAMwAAAD+////AQAAAAAAAAAQAAAABBAAAHwAAAAAAAAABRAAAOgAAAAAAAAAEBAAABQBAAAAAAAAQBAAAAQBAAAAAAAAAiAAAOABAAAAAAAABSAAAFABAAAAAAAAECAAAGABAAAAAAAAESAAAJQBAAAAAAAAEiAAAHQBAAAAAAAAEyAAAIQBAAAAAAAAICAAAMABAAAAAAAAMCAAAOwBAAAAAAAAAcAAADQBAAAAAAAAEcAAABQAAAAAAAAAEsAAAEQBAAAAAAAAgPAAAPQBAAAAAAAAAAgADQAaACMAZQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAARl},
	bdsk-url-1 = {https://arxiv.org/pdf/1412.6980.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1412.6980}}

@misc{SS71,
	author = {D. Scott and C. Strachey},
	date-added = {2024-11-30 13:59:40 +0700},
	date-modified = {2024-11-30 14:00:09 +0700},
	title = {Toward a mathematical semantics for computer languages},
	year = {1971}}

@misc{Sza13,
	author = {Z. G. Szabo ́},
	date-added = {2024-11-30 13:54:44 +0700},
	date-modified = {2024-11-30 13:58:18 +0700},
	editor = {E. N. Zalta},
	howpublished = {The Stanford Encyclopedia of Philosophy (Fall 2013 Edition)},
	keywords = {compositionality},
	organization = {Stanford},
	title = {Compositionality},
	year = {2013}}

@article{shp,
	abstract = {A hypergraph allows a hyperedge to connect more
than two vertices, using which to capture the high-order relationships, many hypergraph learning algorithms are shown
highly effective in various applications. When learning large
hypergraphs, converting them to graphs to employ the distributed
graph frameworks is a common approach, yet it results in major
efficiency drawbacks including an inflated problem size, the
excessive replicas, and the unbalanced workloads. To avoid such
drawbacks, we take a different approach and propose HyperX,
which is a thin layer built upon Spark. To preserve the problem
size, HyperX directly operates on a distributed hypergraph.
To reduce the replicas, HyperX replicates the vertices but not
the hyperedges. To balance the workloads, we investigate the
hypergraph partitioning problem aiming at minimizing the space
and the communication cost subject to two separate constraints
on the hyperedge and the vertex workloads. With experiments
on both real and synthetic datasets, we verify that HyperX
significantly improves the efficiency of the learning algorithms
when compared with the graph conversion approach.},
	author = {Jin Huang and Rui Zhang and Jeffrey Xu Yu},
	bdsk-color = {2},
	date-added = {2024-11-30 13:32:50 +0700},
	date-modified = {2024-11-30 13:36:38 +0700},
	keywords = {hypergraphs, machine learning, graph processing, distributed hypergraph},
	title = {Scalable Hypergraph Processing},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEDIuLi9TdHVkeS9QYXBlcnMvU2NhbGFibGUgSHlwZXJncmFwaCBQcm9jZXNzaW5nLnBkZk8RA+Rib29r5AMAAAAABBAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADgAgAABQAAAAEBAABVc2VycwAAAAYAAAABAQAAc2NhbGVzAAAFAAAAAQEAAFN0dWR5AAAABgAAAAEBAABQYXBlcnMAACIAAAABAQAAU2NhbGFibGUgSHlwZXJncmFwaCBQcm9jZXNzaW5nLnBkZgAAFAAAAAEGAAAEAAAAFAAAACQAAAA0AAAARAAAAAgAAAAEAwAApUEAAAAAAAAIAAAABAMAACnaBQAAAAAACAAAAAQDAAArmCQAAAAAAAgAAAAEAwAAc8dfAAAAAAAIAAAABAMAADs18gEAAAAAFAAAAAEGAACMAAAAnAAAAKwAAAC8AAAAzAAAAAgAAAAABAAAQcZ9dIM3tJoYAAAAAQIAAAEAAAAAAAAADwAAAAAAAAAAAAAAAAAAAAgAAAAEAwAAAwAAAAAAAAAEAAAAAwMAAPcBAAAIAAAAAQkAAGZpbGU6Ly8vDAAAAAEBAABNYWNpbnRvc2ggSEQIAAAABAMAAACQgpbnAAAACAAAAAAEAABBxrzVhQAAACQAAAABAQAANDM4NzY1ODQtNEI2OC00RUEzLUIwMDItRTQ0QTRCOTJBRDRDGAAAAAECAACBAAAAAQAAAO8TAAABAAAAAAAAAAAAAAABAAAAAQEAAC8AAAAAAAAAAQUAAO8AAAABAgAAZjQ3NzVkYTQ5NDhkOTc1MjdlY2Q5Y2M0MjA3OWJmZmQ3YmJlY2EyMDc0OTQwM2M1NGIyYzAwOWRiMDdhMDQyYTswMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDAwMDAwMDIwO2NvbS5hcHBsZS5hcHAtc2FuZGJveC5yZWFkLXdyaXRlOzAxOzAxMDAwMDBmOzAwMDAwMDAwMDFmMjM1M2I7MDE7L3VzZXJzL3NjYWxlcy9zdHVkeS9wYXBlcnMvc2NhbGFibGUgaHlwZXJncmFwaCBwcm9jZXNzaW5nLnBkZgAAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAcAAAAAAAAAAFEAAA3AAAAAAAAAAQEAAACAEAAAAAAABAEAAA+AAAAAAAAAACIAAA1AEAAAAAAAAFIAAARAEAAAAAAAAQIAAAVAEAAAAAAAARIAAAiAEAAAAAAAASIAAAaAEAAAAAAAATIAAAeAEAAAAAAAAgIAAAtAEAAAAAAAAwIAAA4AEAAAAAAAABwAAAKAEAAAAAAAARwAAAFAAAAAAAAAASwAAAOAEAAAAAAACA8AAA6AEAAAAAAAAACAANABoAIwBYAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABEA=}}

@inproceedings{egnnuagtv,
	author = {Qiao, Xueting and Lin, Wanyu and Ouyang, Mingxuan and Jiang, Ting and Zhang, Ji},
	booktitle = {2024 International Joint Conference on Neural Networks (IJCNN)},
	date-added = {2024-11-28 19:12:19 +0700},
	date-modified = {2024-11-28 19:14:21 +0700},
	doi = {10.1109/IJCNN60899.2024.10650495},
	keywords = {Monte Carlo methods;Accuracy;Computational modeling;Games;Graph neural networks;Classification algorithms;Game theory;Graph Neural Networks;Explainability;Games},
	pages = {1-8},
	title = {Explanations for Graph Neural Networks using A Game-theoretic Value},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1109/IJCNN60899.2024.10650495}}

@article{a-fusion,
	author = {Vishnu Priyan S and Vinod Kumar R and Moorthy C and Nishok VS},
	date-added = {2024-11-28 19:07:22 +0700},
	date-modified = {2024-11-28 19:10:09 +0700},
	doi = {10.3233/XST-240027},
	journal = {J Xray Sci Technol},
	keywords = {deep neural networks, generative adversarial networks, game theory},
	number = {4},
	pages = {1011-1039},
	title = {A fusion of deep neural networks and game theory for retinal disease diagnosis with OCT images},
	volume = {32},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.3233/XST-240027}}

@article{pnnucoopgt,
	annote = {GTAP identifies and removes less impactful
neurons based on their contribution to the network's performance,
streamlining the model's size and computational load. },
	author = {Mauricio Diaz-Ortiz Jr and Benjamin Kempinski and Daphne Cornelisse and Yoram Bachrach and Tal Kachman},
	date-added = {2024-11-28 19:05:15 +0700},
	date-modified = {2025-03-09 23:19:19 +0700},
	keywords = {pruning, game theory, cooperative game theory, deep neural networks,},
	title = {Using Cooperative Game Theory to Prune Neural Networks}}

@article{hlctdrl,
	date-added = {2024-11-28 19:04:27 +0700},
	date-modified = {2024-11-28 19:05:09 +0700},
	title = {Human-level control through deep reinforcement learning}}

@article{Varga2017,
	abstract = {Regularizing the gradient norm of the output of a neural network with respect to its inputs is a powerful technique, rediscovered several times. This paper presents evidence that gradient regularization can consistently improve classification accuracy on vision tasks, using modern deep neural networks, especially when the amount of training data is small. We introduce our regularizers as members of a broader class of Jacobian-based regularizers. We demonstrate empirically on real and synthetic data that the learning process leads to gradients controlled beyond the training points, and results in solutions that generalize well.},
	author = {D{\'a}niel Varga and Adri{\'a}n Csisz{\'a}rik and Zsolt Zombori},
	date-added = {2024-11-27 14:22:59 +0700},
	date-modified = {2024-11-27 14:23:06 +0700},
	eprint = {1712.09936},
	month = {12},
	title = {Gradient Regularization Improves Accuracy of Discriminative Models},
	url = {https://arxiv.org/pdf/1712.09936.pdf},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/pdf/1712.09936.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1712.09936}}

@article{Salimans2016,
	abstract = {We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.},
	author = {Tim Salimans and Diederik P. Kingma},
	date-added = {2024-11-27 14:19:10 +0700},
	date-modified = {2024-11-27 14:19:17 +0700},
	eprint = {1602.07868},
	month = {02},
	title = {Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks},
	url = {https://arxiv.org/pdf/1602.07868.pdf},
	year = {2016},
	bdsk-url-1 = {https://arxiv.org/pdf/1602.07868.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1602.07868}}

@article{Gehring2017,
	abstract = {The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.},
	author = {Jonas Gehring and Michael Auli and David Grangier and Denis Yarats and Yann N. Dauphin},
	date-added = {2024-11-27 14:17:40 +0700},
	date-modified = {2024-11-27 14:17:49 +0700},
	eprint = {1705.03122},
	month = {05},
	title = {Convolutional Sequence to Sequence Learning},
	url = {https://arxiv.org/pdf/1705.03122.pdf},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/pdf/1705.03122.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1705.03122}}

@article{Zhang2019aa,
	abstract = {We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption. Under the new condition, we prove that two popular methods, namely, \emph{gradient clipping} and \emph{normalized gradient}, converge arbitrarily faster than gradient descent with fixed stepsize. We further explain why such adaptively scaled gradient methods can accelerate empirical convergence and verify our results empirically in popular neural network training settings.},
	annote = {> Even though gradient clipping is a standard practice in tasks such as language models (e.g.
Merity et al., 2018; Gehring et al., 2017; Peters et al., 2018), it lacks a firm theoretical grounding

},
	author = {Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},
	bdsk-color = {4},
	date-added = {2024-11-27 14:14:07 +0700},
	date-modified = {2024-11-27 14:15:50 +0700},
	eprint = {1905.11881},
	month = {05},
	title = {Why gradient clipping accelerates training: A theoretical justification for adaptivity},
	url = {https://arxiv.org/pdf/1905.11881.pdf},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/pdf/1905.11881.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1905.11881}}

@article{shannon-1950,
	author = {Claude E. Shannon},
	date-added = {2024-11-26 10:50:43 +0700},
	date-modified = {2024-11-26 10:52:28 +0700},
	journal = {Philosophical Magazine},
	month = {March},
	number = {314},
	series = {7},
	title = {Programming a Computer for Playing Chess},
	volume = {41},
	year = {1950},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEDcuLi9TdHVkeS9QYXBlcnMvUHJvZ3JhbW1pbmdhQ29tcHV0ZXJmb3JQbGF5aW5nQ2hlc3MucGRmTxED7GJvb2vsAwAAAAAEEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOgCAAAFAAAAAQEAAFVzZXJzAAAABgAAAAEBAABzY2FsZXMAAAUAAAABAQAAU3R1ZHkAAAAGAAAAAQEAAFBhcGVycwAAJwAAAAEBAABQcm9ncmFtbWluZ2FDb21wdXRlcmZvclBsYXlpbmdDaGVzcy5wZGYAFAAAAAEGAAAEAAAAFAAAACQAAAA0AAAARAAAAAgAAAAEAwAApUEAAAAAAAAIAAAABAMAACnaBQAAAAAACAAAAAQDAAArmCQAAAAAAAgAAAAEAwAAc8dfAAAAAAAIAAAABAMAANXVaQEAAAAAFAAAAAEGAACQAAAAoAAAALAAAADAAAAA0AAAAAgAAAAABAAAQcZo6kMq9I8YAAAAAQIAAAEAAAAAAAAADwAAAAAAAAAAAAAAAAAAAAgAAAAEAwAAAwAAAAAAAAAEAAAAAwMAAPcBAAAIAAAAAQkAAGZpbGU6Ly8vDAAAAAEBAABNYWNpbnRvc2ggSEQIAAAABAMAAACQgpbnAAAACAAAAAAEAABBxrzVhQAAACQAAAABAQAANDM4NzY1ODQtNEI2OC00RUEzLUIwMDItRTQ0QTRCOTJBRDRDGAAAAAECAACBAAAAAQAAAO8TAAABAAAAAAAAAAAAAAABAAAAAQEAAC8AAAAAAAAAAQUAAPQAAAABAgAAMGJmYmM3OWJhZGU4NWUzZDA4NDc0MTc4ZjEwZjEzMjU2ZGMwY2MwMmY5MDdjNjg1N2E2N2NiNzQ0MTI5YjRiYTswMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDAwMDAwMDIwO2NvbS5hcHBsZS5hcHAtc2FuZGJveC5yZWFkLXdyaXRlOzAxOzAxMDAwMDBmOzAwMDAwMDAwMDE2OWQ1ZDU7MDE7L3VzZXJzL3NjYWxlcy9zdHVkeS9wYXBlcnMvcHJvZ3JhbW1pbmdhY29tcHV0ZXJmb3JwbGF5aW5nY2hlc3MucGRmAMwAAAD+////AQAAAAAAAAAQAAAABBAAAHQAAAAAAAAABRAAAOAAAAAAAAAAEBAAAAwBAAAAAAAAQBAAAPwAAAAAAAAAAiAAANgBAAAAAAAABSAAAEgBAAAAAAAAECAAAFgBAAAAAAAAESAAAIwBAAAAAAAAEiAAAGwBAAAAAAAAEyAAAHwBAAAAAAAAICAAALgBAAAAAAAAMCAAAOQBAAAAAAAAAcAAACwBAAAAAAAAEcAAABQAAAAAAAAAEsAAADwBAAAAAAAAgPAAAOwBAAAAAAAAAAgADQAaACMAXQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAARN}}

@article{parnas1978,
	author = {David L. Parnas},
	date-added = {2024-11-26 10:47:49 +0700},
	date-modified = {2024-11-27 14:18:10 +0700},
	title = {Designing Software for Ease of Extension and Contraction},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEwuLi9TdHVkeS9QYXBlcnMvRGVzaWduaW5nIFNvZnR3YXJlIGZvciBFYXNlIG9mIEV4dGVuc2lvbiBhbmQgQ29udHJhY3Rpb24ucGRmTxEEGGJvb2sYBAAAAAAEEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABQDAAAFAAAAAQEAAFVzZXJzAAAABgAAAAEBAABzY2FsZXMAAAUAAAABAQAAU3R1ZHkAAAAGAAAAAQEAAFBhcGVycwAAPAAAAAEBAABEZXNpZ25pbmcgU29mdHdhcmUgZm9yIEVhc2Ugb2YgRXh0ZW5zaW9uIGFuZCBDb250cmFjdGlvbi5wZGYUAAAAAQYAAAQAAAAUAAAAJAAAADQAAABEAAAACAAAAAQDAAClQQAAAAAAAAgAAAAEAwAAKdoFAAAAAAAIAAAABAMAACuYJAAAAAAACAAAAAQDAABzx18AAAAAAAgAAAAEAwAACRwIAAAAAAAUAAAAAQYAAKQAAAC0AAAAxAAAANQAAADkAAAACAAAAAAEAABBxcN5KVlRERgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAADAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAJCClucAAAAIAAAAAAQAAEHGvNWFAAAAJAAAAAEBAAA0Mzg3NjU4NC00QjY4LTRFQTMtQjAwMi1FNDRBNEI5MkFENEMYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAACQEAAAECAAA2YjE5YjRmMzNhNGJlNWVlYjRkZGEzZTY2OWJlNjYwYjIzOTgzYTcxMzQxY2Q0OWUxODAxOWU5YmM5YjdhZWJmOzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMGY7MDAwMDAwMDAwMDA4MWMwOTswMTsvdXNlcnMvc2NhbGVzL3N0dWR5L3BhcGVycy9kZXNpZ25pbmcgc29mdHdhcmUgZm9yIGVhc2Ugb2YgZXh0ZW5zaW9uIGFuZCBjb250cmFjdGlvbi5wZGYAAAAAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAiAAAAAAAAAAFEAAA9AAAAAAAAAAQEAAAIAEAAAAAAABAEAAAEAEAAAAAAAACIAAA7AEAAAAAAAAFIAAAXAEAAAAAAAAQIAAAbAEAAAAAAAARIAAAoAEAAAAAAAASIAAAgAEAAAAAAAATIAAAkAEAAAAAAAAgIAAAzAEAAAAAAAAwIAAA+AEAAAAAAAABwAAAQAEAAAAAAAARwAAAFAAAAAAAAAASwAAAUAEAAAAAAACA8AAAAAIAAAAAAAAACAANABoAIwByAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABI4=},
	bdsk-file-2 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEYuLi8uVHJhc2gvSUVFRSBYcGxvcmUgQ2l0YXRpb24gQmliVGVYIERvd25sb2FkIDIwMjQuMTEuMjguMTkuMTEuNDcuYmliTxED6GJvb2voAwAAAAAEEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOQCAAAFAAAAAQEAAFVzZXJzAAAABgAAAAEBAABzY2FsZXMAAAYAAAABAQAALlRyYXNoAAA8AAAAAQEAAElFRUUgWHBsb3JlIENpdGF0aW9uIEJpYlRlWCBEb3dubG9hZCAyMDI0LjExLjI4LjE5LjExLjQ3LmJpYhAAAAABBgAABAAAABQAAAAkAAAANAAAAAgAAAAEAwAApUEAAAAAAAAIAAAABAMAACnaBQAAAAAACAAAAAQDAADz2gUAAAAAAAgAAAAEAwAAqoHsAQAAAAAQAAAAAQYAAJAAAACgAAAAsAAAAMAAAAAIAAAAAAQAAEHGfErBp1ZpGAAAAAECAAABAAAAAAAAAA8AAAAAAAAAAAAAAAAAAAAIAAAABAMAAAIAAAAAAAAABAAAAAMDAAD3AQAACAAAAAEJAABmaWxlOi8vLwwAAAABAQAATWFjaW50b3NoIEhECAAAAAQDAAAAkIKW5wAAAAgAAAAABAAAQcachScAAAAkAAAAAQEAADQzODc2NTg0LTRCNjgtNEVBMy1CMDAyLUU0NEE0QjkyQUQ0QxgAAAABAgAAgQAAAAEAAADvEwAAAQAAAAAAAAAAAAAAAQAAAAEBAAAvAAAAAAAAAAEFAAADAQAAAQIAADExYTA1NGQxYWJjOTQ3ZGM3M2NhN2YyYTQ4ZWNkMTVmYWQyY2JlMzJiZGRlNzA2YjcyNGIwMjdiYTI2MWJjYWY7MDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDAwMDAwMDAyMDtjb20uYXBwbGUuYXBwLXNhbmRib3gucmVhZC13cml0ZTswMTswMTAwMDAxMDswMDAwMDAwMDAxZWM4MWFhOzg1Oy91c2Vycy9zY2FsZXMvLnRyYXNoL2llZWUgeHBsb3JlIGNpdGF0aW9uIGJpYnRleCBkb3dubG9hZCAyMDI0LjExLjI4LjE5LjExLjQ3LmJpYgAAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAeAAAAAAAAAAFEAAA0AAAAAAAAAAQEAAA+AAAAAAAAABAEAAA6AAAAAAAAAACIAAAxAEAAAAAAAAFIAAANAEAAAAAAAAQIAAARAEAAAAAAAARIAAAeAEAAAAAAAASIAAAWAEAAAAAAAATIAAAaAEAAAAAAAAgIAAApAEAAAAAAAAwIAAA0AEAAAAAAAABwAAAGAEAAAAAAAARwAAAFAAAAAAAAAASwAAAKAEAAAAAAACA8AAA2AEAAAAAAAAACAANABoAIwBsAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABFg=}}

@article{tplpla,
	author = {Shriram Krishnamurthi},
	date-added = {2024-11-26 09:58:55 +0700},
	date-modified = {2024-11-26 09:59:12 +0700},
	title = {Teaching Programming Languages in a Post-Linnaean Age}}

@article{saip,
	date-added = {2024-11-25 17:51:40 +0700},
	date-modified = {2024-11-25 17:52:01 +0700},
	title = {Sociotropy-Autonomy and Interpersonal Problems},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEIuLi9TdHVkeS9QYXBlcnMvU29jaW90cm9weS1BdXRvbm9teSBhbmQgSW50ZXJwZXJzb25hbCBQcm9ibGVtcy5wZGZPEQQEYm9vawQEAAAAAAQQMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMAAAUAAAABAQAAVXNlcnMAAAAGAAAAAQEAAHNjYWxlcwAABQAAAAEBAABTdHVkeQAAAAYAAAABAQAAUGFwZXJzAAAyAAAAAQEAAFNvY2lvdHJvcHktQXV0b25vbXkgYW5kIEludGVycGVyc29uYWwgUHJvYmxlbXMucGRmAAAUAAAAAQYAAAQAAAAUAAAAJAAAADQAAABEAAAACAAAAAQDAAClQQAAAAAAAAgAAAAEAwAAKdoFAAAAAAAIAAAABAMAACuYJAAAAAAACAAAAAQDAABzx18AAAAAAAgAAAAEAwAAQziuAQAAAAAUAAAAAQYAAJwAAACsAAAAvAAAAMwAAADcAAAACAAAAAAEAABBxnJCeTVgsBgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAADAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAJCClucAAAAIAAAAAAQAAEHGvNWFAAAAJAAAAAEBAAA0Mzg3NjU4NC00QjY4LTRFQTMtQjAwMi1FNDRBNEI5MkFENEMYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAA/wAAAAECAABmMDgwYjNkNmZiZGY4NDBhYjE1ZjQ3MTNmMGEzODY0NzM1ZmFmODY2YWI2YmRjMzU0ZDYxNWViZGYzNDQ5MmY4OzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMGY7MDAwMDAwMDAwMWFlMzg0MzswMTsvdXNlcnMvc2NhbGVzL3N0dWR5L3BhcGVycy9zb2Npb3Ryb3B5LWF1dG9ub215IGFuZCBpbnRlcnBlcnNvbmFsIHByb2JsZW1zLnBkZgAAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAgAAAAAAAAAAFEAAA7AAAAAAAAAAQEAAAGAEAAAAAAABAEAAACAEAAAAAAAACIAAA5AEAAAAAAAAFIAAAVAEAAAAAAAAQIAAAZAEAAAAAAAARIAAAmAEAAAAAAAASIAAAeAEAAAAAAAATIAAAiAEAAAAAAAAgIAAAxAEAAAAAAAAwIAAA8AEAAAAAAAABwAAAOAEAAAAAAAARwAAAFAAAAAAAAAASwAAASAEAAAAAAACA8AAA+AEAAAAAAAAACAANABoAIwBoAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABHA=}}

@article{friedman2000,
	author = {Jerome H. Friedman},
	date-added = {2024-11-25 16:28:47 +0700},
	date-modified = {2024-11-25 16:30:05 +0700},
	journal = {The Annals of Statistics},
	month = {November},
	number = {5},
	pages = {1189-1232},
	title = {Greedy Function Approximation: A Gradient Boosting Machine},
	volume = {29},
	year = {2000},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEYuLi9TdHVkeS9QYXBlcnMvR3JlZWR5X0Z1bmN0aW9uX0FwcHJveGltYXRpb25fQV9HcmFkaWVudF9Cb29zdGluZ18ucGRmTxEESGJvb2tIBAAAAAAEEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADgDAAAFAAAAAQEAAFVzZXJzAAAABgAAAAEBAABzY2FsZXMAAAUAAAABAQAAU3R1ZHkAAAAGAAAAAQEAAFBhcGVycwAANgAAAAEBAABHcmVlZHlfRnVuY3Rpb25fQXBwcm94aW1hdGlvbl9BX0dyYWRpZW50X0Jvb3N0aW5nXy5wZGYAABQAAAABBgAABAAAABQAAAAkAAAANAAAAEQAAAAIAAAABAMAAKVBAAAAAAAACAAAAAQDAAAp2gUAAAAAAAgAAAAEAwAAK5gkAAAAAAAIAAAABAMAAHPHXwAAAAAACAAAAAQDAACmDhgBAAAAABQAAAABBgAAoAAAALAAAADAAAAA0AAAAOAAAAAIAAAAAAQAAEHGW7yg6jv1GAAAAAECAAABAAAAAAAAAA8AAAAAAAAAAAAAAAAAAAAIAAAABAMAAAMAAAAAAAAABAAAAAMDAAD3AQAACAAAAAEJAABmaWxlOi8vLwwAAAABAQAATWFjaW50b3NoIEhECAAAAAQDAAAAkIKW5wAAAAgAAAAABAAAQca81YUAAAAkAAAAAQEAADQzODc2NTg0LTRCNjgtNEVBMy1CMDAyLUU0NEE0QjkyQUQ0QxgAAAABAgAAgQAAAAEAAADvEwAAAQAAAAAAAAAAAAAAAQAAAAEBAAAvAAAAAAAAAAEFAAAaAAAAAQEAAE5TVVJMRG9jdW1lbnRJZGVudGlmaWVyS2V5AAAEAAAAAwMAAFM4AQADAQAAAQIAADJmMWI5MWRlODVkYWQ0YTBlMDkxMTFmMDg2YTc4NGZhZjRiZDVmZDViOWM4MTRkZmRmNDYyNmIzYTVjNTZjNDc7MDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDAwMDAwMDAyMDtjb20uYXBwbGUuYXBwLXNhbmRib3gucmVhZC13cml0ZTswMTswMTAwMDAwZjswMDAwMDAwMDAxMTgwZWE2OzAxOy91c2Vycy9zY2FsZXMvc3R1ZHkvcGFwZXJzL2dyZWVkeV9mdW5jdGlvbl9hcHByb3hpbWF0aW9uX2FfZ3JhZGllbnRfYm9vc3RpbmdfLnBkZgAA2AAAAP7///8BAAAAAAAAABEAAAAEEAAAhAAAAAAAAAAFEAAA8AAAAAAAAAAQEAAAHAEAAAAAAABAEAAADAEAAAAAAAACIAAA6AEAAAAAAAAFIAAAWAEAAAAAAAAQIAAAaAEAAAAAAAARIAAAnAEAAAAAAAASIAAAfAEAAAAAAAATIAAAjAEAAAAAAAAgIAAAyAEAAAAAAAAwIAAA9AEAAAAAAAABwAAAPAEAAAAAAAARwAAAFAAAAAAAAAASwAAATAEAAAAAAACA8AAALAIAAAAAAAD8AQCAIAIAAAAAAAAACAANABoAIwBsAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABLg=}}

@book{100pmlb,
	date-added = {2024-11-25 00:11:09 +0700},
	date-modified = {2024-11-25 00:11:40 +0700},
	title = {The Hundred-Page Machine Learning Book},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEGAuLi9MaWJyYXJ5L01vYmlsZSBEb2N1bWVudHMvY29tfmFwcGxlfkNsb3VkRG9jcy9Cb29rcy9UaGUgSHVuZHJlZC1QYWdlIE1hY2hpbmUgTGVhcm5pbmcgQm9vay5wZGZPEQS8Ym9va7wEAAAAAAQQMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAArAMAAAUAAAABAQAAVXNlcnMAAAAGAAAAAQEAAHNjYWxlcwAABwAAAAEBAABMaWJyYXJ5ABAAAAABAQAATW9iaWxlIERvY3VtZW50cxMAAAABAQAAY29tfmFwcGxlfkNsb3VkRG9jcwAFAAAAAQEAAEJvb2tzAAAAKgAAAAEBAABUaGUgSHVuZHJlZC1QYWdlIE1hY2hpbmUgTGVhcm5pbmcgQm9vay5wZGYAABwAAAABBgAABAAAABQAAAAkAAAANAAAAEwAAABoAAAAeAAAAAgAAAAEAwAApUEAAAAAAAAIAAAABAMAACnaBQAAAAAACAAAAAQDAAAr/RQAAAAAAAgAAAAEAwAAB9UjAAAAAAAIAAAABAMAAGLVIwAAAAAACAAAAAQDAABfcCQAAAAAAAgAAAAEAwAAumOEBAAAAAAcAAAAAQYAANAAAADgAAAA8AAAAAABAAAQAQAAIAEAADABAAAIAAAAAAQAAEHF895CVa9sGAAAAAECAAABAAAAAAAAAA8AAAAAAAAAAAAAAAAAAAAIAAAABAMAAAUAAAAAAAAABAAAAAMDAAD3AQAACAAAAAEJAABmaWxlOi8vLwwAAAABAQAATWFjaW50b3NoIEhECAAAAAQDAAAAkIKW5wAAAAgAAAAABAAAQca81YUAAAAkAAAAAQEAADQzODc2NTg0LTRCNjgtNEVBMy1CMDAyLUU0NEE0QjkyQUQ0QxgAAAABAgAAgQAAAAEAAADvEwAAAQAAAAAAAAAAAAAAAQAAAAEBAAAvAAAAAAAAAAEFAAAaAAAAAQEAAE5TVVJMRG9jdW1lbnRJZGVudGlmaWVyS2V5AAAEAAAAAwMAAEkAAAAdAQAAAQIAADRlNDI0ZDMwNWRiYmFkNTg5NjNkYTFkOWIzYTQ1MzAxMzU0YTI4M2Y4ZjkwZDcwNDZiZjA2NWRiNjA1YWMwZDc7MDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDAwMDAwMDAyMDtjb20uYXBwbGUuYXBwLXNhbmRib3gucmVhZC13cml0ZTswMTswMTAwMDAwZjswMDAwMDAwMDA0ODQ2M2JhOzUxOy91c2Vycy9zY2FsZXMvbGlicmFyeS9tb2JpbGUgZG9jdW1lbnRzL2NvbX5hcHBsZX5jbG91ZGRvY3MvYm9va3MvdGhlIGh1bmRyZWQtcGFnZSBtYWNoaW5lIGxlYXJuaW5nIGJvb2sucGRmAAAAANgAAAD+////AQAAAAAAAAARAAAABBAAAKwAAAAAAAAABRAAAEABAAAAAAAAEBAAAHQBAAAAAAAAQBAAAGQBAAAAAAAAAiAAAEACAAAAAAAABSAAALABAAAAAAAAECAAAMABAAAAAAAAESAAAPQBAAAAAAAAEiAAANQBAAAAAAAAEyAAAOQBAAAAAAAAICAAACACAAAAAAAAMCAAAEwCAAAAAAAAAcAAAJQBAAAAAAAAEcAAABQAAAAAAAAAEsAAAKQBAAAAAAAAgPAAAIQCAAAAAAAAVAIAgHgCAAAAAAAAAAgADQAaACMAhgAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAVG}}

@article{Butlin2023,
	abstract = {Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive "indicator properties" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also suggests that there are no obvious technical barriers to building AI systems which satisfy these indicators.},
	author = {Patrick Butlin and Robert Long and Eric Elmoznino and Yoshua Bengio and Jonathan Birch and Axel Constant and George Deane and Stephen M. Fleming and Chris Frith and Xu Ji and Ryota Kanai and Colin Klein and Grace Lindsay and Matthias Michel and Liad Mudrik and Megan A. K. Peters and Eric Schwitzgebel and Jonathan Simon and Rufin VanRullen},
	date-added = {2024-11-24 23:44:14 +0700},
	date-modified = {2024-11-27 14:18:05 +0700},
	eprint = {2308.08708},
	keywords = {consciousness, artificial intelligense},
	month = {08},
	title = {Consciousness in Artificial Intelligence: Insights from the Science of Consciousness},
	url = {https://arxiv.org/pdf/2308.08708.pdf},
	year = {2023},
	bdsk-url-1 = {https://arxiv.org/pdf/2308.08708.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/2308.08708}}

@article{santoro16,
	author = {Adam Santoro and Sergey Bartunov and Matthew Botvinick and Daan Wierstra and Timothy Lillicrap},
	date-added = {2024-11-24 23:34:14 +0700},
	date-modified = {2024-11-24 23:35:26 +0700},
	journal = {Proceedings of the 33 rd International Conference on Machine Learning},
	title = {Meta-Learning with Memory-Augmented Neural Networks},
	volume = {48},
	year = {2016},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEcuLi9TdHVkeS9QYXBlcnMvTWV0YS1MZWFybmluZyB3aXRoIE1lbW9yeS1BdWdtZW50ZWQgTmV1cmFsIE5ldHdvcmtzLnBkZk8RBAxib29rDAQAAAAABBAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAwAABQAAAAEBAABVc2VycwAAAAYAAAABAQAAc2NhbGVzAAAFAAAAAQEAAFN0dWR5AAAABgAAAAEBAABQYXBlcnMAADcAAAABAQAATWV0YS1MZWFybmluZyB3aXRoIE1lbW9yeS1BdWdtZW50ZWQgTmV1cmFsIE5ldHdvcmtzLnBkZgAUAAAAAQYAAAQAAAAUAAAAJAAAADQAAABEAAAACAAAAAQDAAClQQAAAAAAAAgAAAAEAwAAKdoFAAAAAAAIAAAABAMAACuYJAAAAAAACAAAAAQDAABzx18AAAAAAAgAAAAEAwAANGHaAQAAAAAUAAAAAQYAAKAAAACwAAAAwAAAANAAAADgAAAACAAAAAAEAABBxnnGt5EL2xgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAADAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAJCClucAAAAIAAAAAAQAAEHGvNWFAAAAJAAAAAEBAAA0Mzg3NjU4NC00QjY4LTRFQTMtQjAwMi1FNDRBNEI5MkFENEMYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAABAEAAAECAAA0MWUyY2U4NDE2ZWJlNDcyOWMwOWViYTAzYzJmOGMxNzhiMjc3NjRkZjExODU5YTQ5NWM2M2MzZDBhMzdkYzVhOzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMGY7MDAwMDAwMDAwMWRhNjEzNDswMTsvdXNlcnMvc2NhbGVzL3N0dWR5L3BhcGVycy9tZXRhLWxlYXJuaW5nIHdpdGggbWVtb3J5LWF1Z21lbnRlZCBuZXVyYWwgbmV0d29ya3MucGRmAMwAAAD+////AQAAAAAAAAAQAAAABBAAAIQAAAAAAAAABRAAAPAAAAAAAAAAEBAAABwBAAAAAAAAQBAAAAwBAAAAAAAAAiAAAOgBAAAAAAAABSAAAFgBAAAAAAAAECAAAGgBAAAAAAAAESAAAJwBAAAAAAAAEiAAAHwBAAAAAAAAEyAAAIwBAAAAAAAAICAAAMgBAAAAAAAAMCAAAPQBAAAAAAAAAcAAADwBAAAAAAAAEcAAABQAAAAAAAAAEsAAAEwBAAAAAAAAgPAAAPwBAAAAAAAAAAgADQAaACMAbQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAR9}}

@article{einstein1905,
	author = {Einstein, Albert},
	journal = {Annalen der Physik},
	number = {10},
	pages = {891--921},
	publisher = {Wiley Online Library},
	title = {Zur Elektrodynamik bewegter K{\"o}rper},
	volume = {322},
	year = {1905}}

@book{dirac1930,
	author = {Dirac, Paul A. M.},
	publisher = {Clarendon Press},
	title = {The Principles of Quantum Mechanics},
	year = {1930}}

@electronic{cegt-ppt,
	annote = {Provides very good examples of open games},
	author = {Neil Ghani and Julian Hedges and Viktor Winschel and Philipp Zahn},
	date-added = {2024-11-23 16:40:12 +0700},
	date-modified = {2024-11-23 17:00:05 +0700},
	howpublished = {Website: https://conferences.inf.ed.ac.uk/ct2019/slides/ghani.pdf},
	keywords = {compositional game theory, category theory},
	month = {July},
	organization = {Edinburgh},
	title = {Compositional Economic Game Theory},
	year = {2019},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfECQuLi9TdHVkeS9QYXBlcnMvQ0dUIFByZXNlbnRhdGlvbi5wZGZPEQQEYm9vawQEAAAAAAQQMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA9AIAAAUAAAABAQAAVXNlcnMAAAAGAAAAAQEAAHNjYWxlcwAABQAAAAEBAABTdHVkeQAAAAYAAAABAQAAUGFwZXJzAAAUAAAAAQEAAENHVCBQcmVzZW50YXRpb24ucGRmFAAAAAEGAAAEAAAAFAAAACQAAAA0AAAARAAAAAgAAAAEAwAApUEAAAAAAAAIAAAABAMAACnaBQAAAAAACAAAAAQDAAArmCQAAAAAAAgAAAAEAwAAc8dfAAAAAAAIAAAABAMAAJvJ2QEAAAAAFAAAAAEGAAB8AAAAjAAAAJwAAACsAAAAvAAAAAgAAAAABAAAQcZ47KARduMYAAAAAQIAAAEAAAAAAAAADwAAAAAAAAAAAAAAAAAAAAgAAAAEAwAAAwAAAAAAAAAEAAAAAwMAAPcBAAAIAAAAAQkAAGZpbGU6Ly8vDAAAAAEBAABNYWNpbnRvc2ggSEQIAAAABAMAAACQgpbnAAAACAAAAAAEAABBxrzVhQAAACQAAAABAQAANDM4NzY1ODQtNEI2OC00RUEzLUIwMDItRTQ0QTRCOTJBRDRDGAAAAAECAACBAAAAAQAAAO8TAAABAAAAAAAAAAAAAAABAAAAAQEAAC8AAAAAAAAAAQUAABoAAAABAQAATlNVUkxEb2N1bWVudElkZW50aWZpZXJLZXkAAAQAAAADAwAA6D0BAOEAAAABAgAAY2UzOTYwODhmYmVlYTU3Yjc4YmU0ODE3MmQ4MjQyYTQ4NGMzZTA4ZmZhYTY5ZDVlMjNmMjY1MzdhYzIyOWQ3MjswMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDAwMDAwMDIwO2NvbS5hcHBsZS5hcHAtc2FuZGJveC5yZWFkLXdyaXRlOzAxOzAxMDAwMDBmOzAwMDAwMDAwMDFkOWM5OWI7MDE7L3VzZXJzL3NjYWxlcy9zdHVkeS9wYXBlcnMvY2d0IHByZXNlbnRhdGlvbi5wZGYAAAAA2AAAAP7///8BAAAAAAAAABEAAAAEEAAAYAAAAAAAAAAFEAAAzAAAAAAAAAAQEAAA+AAAAAAAAABAEAAA6AAAAAAAAAACIAAAxAEAAAAAAAAFIAAANAEAAAAAAAAQIAAARAEAAAAAAAARIAAAeAEAAAAAAAASIAAAWAEAAAAAAAATIAAAaAEAAAAAAAAgIAAApAEAAAAAAAAwIAAA0AEAAAAAAAABwAAAGAEAAAAAAAARwAAAFAAAAAAAAAASwAAAKAEAAAAAAACA8AAACAIAAAAAAADYAQCA/AEAAAAAAAAACAANABoAIwBKAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABFI=}}

@article{ctrx,
	author = {олмстед},
	date-added = {2024-11-22 23:22:56 +0700},
	date-modified = {2024-11-22 23:23:14 +0700},
	title = {контрпримеры в анализе}}

@article{cat-theory,
	date-added = {2024-11-22 11:32:50 +0700},
	date-modified = {2024-11-22 11:33:02 +0700},
	title = {Учебник по теории категорий}}

@article{c4log,
	abstract = {Categories of polymorphic lenses in computer science, and of open games in compositional game theory, have a curious structure that is reminiscent of compact closed categories, but differs in some crucial ways. Specifically they have a family of morphisms that behave like the counits of a compact closed category, but have no corresponding units; and they have a `partial' duality that behaves like transposition in a compact closed category when it is defined. We axiomatise this structure, which we refer to as a `teleological category'. We precisely define a diagrammatic language suitable for these categories, and prove a coherence theorem for them. This underpins the use of diagrammatic reasoning in compositional game theory, which has previously been used only informally.},
	annote = {- formal presentation of string diagrams
- proves a coherence theorem by which we can define an open game by its string diagram},
	author = {Jules Hedges},
	bdsk-color = {3},
	date-added = {2024-11-19 17:03:44 +0700},
	date-modified = {2024-11-19 17:05:31 +0700},
	eprint = {1704.02230},
	month = {04},
	title = {Coherence for lenses and open games},
	url = {https://arxiv.org/pdf/1704.02230.pdf},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/pdf/1704.02230.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1704.02230}}

@article{abscaloops,
	author = {Samson Abramsky},
	bdsk-color = {4289007103},
	date-added = {2024-11-19 12:20:31 +0700},
	date-modified = {2024-11-19 12:21:48 +0700},
	journal = {Lectures notes in computer science},
	pages = {1-29},
	title = {Abstract scalars, loops, and free traced and strongly compact closed categories},
	volume = {3629},
	year = {2005}}

@article{proptics,
	abstract = {Data accessors allow one to read and write components of a data structure, such as the fields of a
record, the variants of a union, or the elements of a container. These data accessors are collectively known as
optics; they are fundamental to programs that manipulate complex data. Individual data accessors for simple
data structures are easy to write, for example as pairs of `getter' and `setter' methods. However, it is not
obvious how to combine data accessors, in such a way that data accessors for a compound data structure are
composed out of smaller data accessors for the parts of that structure. Generally, one has to write a sequence
of statements or declarations that navigate step by step through the data structure, accessing one level at a
time---which is to say, data accessors are traditionally not first-class citizens, combinable in their own right.
We present a framework for modular data access, in which individual data accessors for simple data structures may be freely combined to obtain more complex data accessors for compound data structures. Data
accessors become first-class citizens. The framework is based around the notion of profunctors, a flexible generalization of functions. The language features required are higher-order functions (`lambdas' or `closures'),
parametrized types (`generics' or `abstract types') of higher kind, and some mechanism for separating interfaces from implementations (`abstract classes' or `modules'). We use Haskell as a vehicle in which to present
our constructions, but other languages such as Scala that provide the necessary features should work just as
well. We provide implementations of all our constructions, in the form of a literate program: the manuscript
file for the paper is also the source code for the program, and the extracted code is available separately for
evaluation. We also prove the essential properties, demonstrating that our profunctor-based representations
are precisely equivalent to the more familiar concrete representations. Our results should pave the way to
simpler ways of writing programs that access the components of compound data structures.},
	annote = {Polymorphic lenses},
	author = {Matthew Pickering and Jeremy Gibbons and Nicolas Wu},
	bdsk-color = {3},
	date-added = {2024-11-19 12:15:22 +0700},
	date-modified = {2024-12-17 09:46:56 +0700},
	journal = {The art, science and engineering of programming},
	keywords = {lens, traversal, compositionality, optics, data accessors, profunctors, modular data access},
	number = {2},
	title = {Profunctor optics: Modular data accessors},
	volume = {1},
	year = {2017},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfED4uLi9TdHVkeS9QYXBlcnMvUHJvZnVuY3RvciBPcHRpY3MgLSBNb2R1bGFyIERhdGEgQWNjZXNzb3JzLnBkZk8RA/xib29r/AMAAAAABBAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD4AgAABQAAAAEBAABVc2VycwAAAAYAAAABAQAAc2NhbGVzAAAFAAAAAQEAAFN0dWR5AAAABgAAAAEBAABQYXBlcnMAAC4AAAABAQAAUHJvZnVuY3RvciBPcHRpY3MgLSBNb2R1bGFyIERhdGEgQWNjZXNzb3JzLnBkZgAAFAAAAAEGAAAEAAAAFAAAACQAAAA0AAAARAAAAAgAAAAEAwAApUEAAAAAAAAIAAAABAMAACnaBQAAAAAACAAAAAQDAAArmCQAAAAAAAgAAAAEAwAAc8dfAAAAAAAIAAAABAMAAMQwFQIAAAAAFAAAAAEGAACYAAAAqAAAALgAAADIAAAA2AAAAAgAAAAABAAAQcaIjiZmjQYYAAAAAQIAAAEAAAAAAAAADwAAAAAAAAAAAAAAAAAAAAgAAAAEAwAAAwAAAAAAAAAEAAAAAwMAAPcBAAAIAAAAAQkAAGZpbGU6Ly8vDAAAAAEBAABNYWNpbnRvc2ggSEQIAAAABAMAAACQgpbnAAAACAAAAAAEAABBxrzVhQAAACQAAAABAQAANDM4NzY1ODQtNEI2OC00RUEzLUIwMDItRTQ0QTRCOTJBRDRDGAAAAAECAACBAAAAAQAAAO8TAAABAAAAAAAAAAAAAAABAAAAAQEAAC8AAAAAAAAAAQUAAPsAAAABAgAAN2NkNWI4OTc1YWViOWE4NDJhNDc5OTljZGFkYjdhNTNiNDMzZDdiMzM2NzQ2ZWJmNDBjOGZiN2Q4YWE1YmY4ZjswMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDAwMDAwMDIwO2NvbS5hcHBsZS5hcHAtc2FuZGJveC5yZWFkLXdyaXRlOzAxOzAxMDAwMDBmOzAwMDAwMDAwMDIxNTMwYzQ7MDE7L3VzZXJzL3NjYWxlcy9zdHVkeS9wYXBlcnMvcHJvZnVuY3RvciBvcHRpY3MgLSBtb2R1bGFyIGRhdGEgYWNjZXNzb3JzLnBkZgAAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAfAAAAAAAAAAFEAAA6AAAAAAAAAAQEAAAFAEAAAAAAABAEAAABAEAAAAAAAACIAAA4AEAAAAAAAAFIAAAUAEAAAAAAAAQIAAAYAEAAAAAAAARIAAAlAEAAAAAAAASIAAAdAEAAAAAAAATIAAAhAEAAAAAAAAgIAAAwAEAAAAAAAAwIAAA7AEAAAAAAAABwAAANAEAAAAAAAARwAAAFAAAAAAAAAASwAAARAEAAAAAAACA8AAA9AEAAAAAAAAACAANABoAIwBkAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABGQ=}}

@phdthesis{decprop-nn,
	author = {Maria Zaitseva},
	bdsk-color = {4291887103},
	date-added = {2024-11-18 16:07:27 +0700},
	date-modified = {2025-03-02 23:06:37 +0700},
	keywords = {compositional game theory, neural networks, open games, applied category theory, categorical cybernetics},
	rating = {5},
	school = {ITMO},
	title = {Lambda the Ultimate Open Game},
	year = {2027}}

@article{seqg-opts,
	annote = {Selection functions},
	author = {Martin Escard{\'o} and Paulo Oliva},
	bdsk-color = {2},
	date-added = {2024-11-18 11:55:22 +0700},
	date-modified = {2024-11-19 12:03:02 +0700},
	journal = {Proceedings of the Royal Society A},
	pages = {1519-1545},
	title = {Sequential games and optimal strategies},
	volume = {467},
	year = {2011}}

@book{es-gt,
	abstract = {Game theory is the mathematical study of interaction among independent, self-interested
agents. The audience for game theory has grown dramatically in recent years, and now spans
disciplines as diverse as political science, biology, psychology, economics, linguistics, sociology
and computer science--among others. What has been missing is a relatively short introduction
to the field covering the common basis that anyone with a professional interest in game theory
is likely to require. Such a text would minimize notation, ruthlessly focus on essentials, and
yet not sacrifice rigor. This Synthesis Lecture aims to fill this gap by providing a concise and
accessible introduction to the field. It covers the main classes of games, their representations,
and the main concepts used to analyze them.},
	annote = {Game theory introduction},
	author = {Kevin Leyton-Brown and Yoav Shoham},
	bdsk-color = {9895783},
	date-added = {2024-11-18 11:54:05 +0700},
	date-modified = {2024-12-18 15:30:10 +0700},
	keywords = {game theory, multiagent systems, competition, coordination, prisoner's dilemma, zero-sum games, nash equilibrium, extensive form, repeated games, stochastic games, bayesian games, coalitional games},
	publisher = {Morgan and Claypool},
	rating = {5},
	title = {Essentials of game theory: a concise, multidisciplinary introduction},
	year = {2008},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEFYuLi9TdHVkeS9QYXBlcnMvRXNzZW50aWFscy1vZi1HYW1lLVRoZW9yeS1BLUNvbmNpc2UtTXVsdGlkaXNjaXBsaW5hcnktSW50cm9kdWN0aW9uLnBkZk8RBCxib29rLAQAAAAABBAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAoAwAABQAAAAEBAABVc2VycwAAAAYAAAABAQAAc2NhbGVzAAAFAAAAAQEAAFN0dWR5AAAABgAAAAEBAABQYXBlcnMAAEYAAAABAQAARXNzZW50aWFscy1vZi1HYW1lLVRoZW9yeS1BLUNvbmNpc2UtTXVsdGlkaXNjaXBsaW5hcnktSW50cm9kdWN0aW9uLnBkZgAAFAAAAAEGAAAEAAAAFAAAACQAAAA0AAAARAAAAAgAAAAEAwAApUEAAAAAAAAIAAAABAMAACnaBQAAAAAACAAAAAQDAAArmCQAAAAAAAgAAAAEAwAAc8dfAAAAAAAIAAAABAMAAECi8gEAAAAAFAAAAAEGAACwAAAAwAAAANAAAADgAAAA8AAAAAgAAAAABAAAQcZ9oemGJiYYAAAAAQIAAAEAAAAAAAAADwAAAAAAAAAAAAAAAAAAAAgAAAAEAwAAAwAAAAAAAAAEAAAAAwMAAPcBAAAIAAAAAQkAAGZpbGU6Ly8vDAAAAAEBAABNYWNpbnRvc2ggSEQIAAAABAMAAACQgpbnAAAACAAAAAAEAABBxrzVhQAAACQAAAABAQAANDM4NzY1ODQtNEI2OC00RUEzLUIwMDItRTQ0QTRCOTJBRDRDGAAAAAECAACBAAAAAQAAAO8TAAABAAAAAAAAAAAAAAABAAAAAQEAAC8AAAAAAAAAAQUAABMBAAABAgAAMzBmNWM0MDI0Y2EzMTM4NmM3MTk0NTAyZGQ3MGM2YzZiODI5NDU1OGExZGEzOTA0MWVlYzgxMjc3ZTNjNjcyZTswMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDAwMDAwMDIwO2NvbS5hcHBsZS5hcHAtc2FuZGJveC5yZWFkLXdyaXRlOzAxOzAxMDAwMDBmOzAwMDAwMDAwMDFmMmEyNDA7MDE7L3VzZXJzL3NjYWxlcy9zdHVkeS9wYXBlcnMvZXNzZW50aWFscy1vZi1nYW1lLXRoZW9yeS1hLWNvbmNpc2UtbXVsdGlkaXNjaXBsaW5hcnktaW50cm9kdWN0aW9uLnBkZgAAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAlAAAAAAAAAAFEAAAAAEAAAAAAAAQEAAALAEAAAAAAABAEAAAHAEAAAAAAAACIAAA+AEAAAAAAAAFIAAAaAEAAAAAAAAQIAAAeAEAAAAAAAARIAAArAEAAAAAAAASIAAAjAEAAAAAAAATIAAAnAEAAAAAAAAgIAAA2AEAAAAAAAAwIAAABAIAAAAAAAABwAAATAEAAAAAAAARwAAAFAAAAAAAAAASwAAAXAEAAAAAAACA8AAADAIAAAAAAAAACAANABoAIwB8AAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABKw=}}

@misc{moog,
	author = {Jules Hedges},
	bdsk-color = {3},
	date-added = {2024-11-18 11:50:13 +0700},
	date-modified = {2024-11-18 11:50:53 +0700},
	howpublished = {arXiv:1711.07059},
	title = {Morphisms of open games},
	year = {2017}}

@article{hiorderdecth,
	annote = {Selection functions!!
Higher order decision theory
Algorithmic decision theory},
	author = {Jules Hedges and Paulo Oliva and Evguenia Shprits and Viktor Winschel and Philipp Zahn},
	bdsk-color = {3},
	date-added = {2024-11-18 11:46:02 +0700},
	date-modified = {2024-11-19 12:13:42 +0700},
	journal = {Lecture Notes in Artificial Intelligence},
	pages = {241-254},
	title = {Higher-order decision theory},
	volume = {10576},
	year = {2017}}

@misc{coalgebra-sem,
	annote = {Treats game dynamics (?) compositionally},
	author = {Achim Blumensath and Viktor Winschel},
	bdsk-color = {3},
	date-added = {2024-11-18 11:41:15 +0700},
	date-modified = {2024-11-18 11:44:55 +0700},
	howpublished = {arXiv:1712.08381},
	title = {A compositional coalgebraic semantics of strategic games},
	year = {2013}}

@article{semapeqr,
	annote = {Discusses games as processes},
	author = {Dusko Pavlovic},
	bdsk-color = {3},
	date-added = {2024-11-18 11:36:47 +0700},
	date-modified = {2024-11-30 19:51:51 +0700},
	journal = {Lectures notes in computer science},
	pages = {317-334},
	title = {A Semantical Approach to Equilibria and Rationality},
	volume = {5728},
	year = {2008}}

@phdthesis{aois,
	abstract = {Herein we develop category-theoretic tools for understanding network-style diagrammatic languages. The archetypal network-style diagram- matic language is that of electric circuits; other examples include signal flow graphs, Markov processes, automata, Petri nets, chemical reaction networks, and so on. The key feature is that the language is comprised of a number of components with multiple (input/output) terminals, each possibly labelled with some type, that may then be connected together along these terminals to form a larger network. The components form hyperedges between labelled vertices, and so a diagram in this language forms a hypergraph. We formalise the compositional structure by intro- ducing the notion of a hypergraph category. Network-style diagrammatic languages and their semantics thus form hypergraph categories, and se- mantic interpretation gives a hypergraph functor.
The first part of this thesis develops the theory of hypergraph categories. In particular, we introduce the tools of decorated cospans and corela- tions. Decorated cospans allow straightforward construction of hyper- graph categories from diagrammatic languages: the inputs, outputs, and their composition are modelled by the cospans, while the `decorations' specify the components themselves. Not all hypergraph categories can be constructed, however, through decorated cospans. Decorated corelations are a more powerful version that permits construction of all hypergraph categories and hypergraph functors. These are often useful for construct- ing the semantic categories of diagrammatic languages and functors from diagrams to the semantics. To illustrate these principles, the second part of this thesis details applications to linear time-invariant dynamical sys- tems and passive linear networks.},
	annote = {Discusses categorical open systems},
	author = {Brendan Fong},
	bdsk-color = {3},
	date-added = {2024-11-18 11:30:35 +0700},
	date-modified = {2024-11-30 13:37:15 +0700},
	school = {University of Oxford},
	title = {The Algebra of Open and Interconnected Systems},
	year = {2016},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEIuLi9TdHVkeS9QYXBlcnMvVGhlIEFsZ2VicmEgb2YgT3BlbiBhbmQgSW50ZXJjb25uZWN0ZWQgU3lzdGVtcy5wZGZPEQQEYm9vawQEAAAAAAQQMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMAAAUAAAABAQAAVXNlcnMAAAAGAAAAAQEAAHNjYWxlcwAABQAAAAEBAABTdHVkeQAAAAYAAAABAQAAUGFwZXJzAAAyAAAAAQEAAFRoZSBBbGdlYnJhIG9mIE9wZW4gYW5kIEludGVyY29ubmVjdGVkIFN5c3RlbXMucGRmAAAUAAAAAQYAAAQAAAAUAAAAJAAAADQAAABEAAAACAAAAAQDAAClQQAAAAAAAAgAAAAEAwAAKdoFAAAAAAAIAAAABAMAACuYJAAAAAAACAAAAAQDAABzx18AAAAAAAgAAAAEAwAABi/+AQAAAAAUAAAAAQYAAJwAAACsAAAAvAAAAMwAAADcAAAACAAAAAAEAABBxn1yliqUCRgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAADAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAJCClucAAAAIAAAAAAQAAEHGvNWFAAAAJAAAAAEBAAA0Mzg3NjU4NC00QjY4LTRFQTMtQjAwMi1FNDRBNEI5MkFENEMYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAA/wAAAAECAAA4NDA1MTdjNTE1ZDIwZjQxOTg4NDYzMzIxYzkxNWQ0M2VmOTk0Zjk5NWIwZjkyZTgzZmVkZDkwNzFhODlkMDJiOzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMGY7MDAwMDAwMDAwMWZlMmYwNjswMTsvdXNlcnMvc2NhbGVzL3N0dWR5L3BhcGVycy90aGUgYWxnZWJyYSBvZiBvcGVuIGFuZCBpbnRlcmNvbm5lY3RlZCBzeXN0ZW1zLnBkZgAAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAgAAAAAAAAAAFEAAA7AAAAAAAAAAQEAAAGAEAAAAAAABAEAAACAEAAAAAAAACIAAA5AEAAAAAAAAFIAAAVAEAAAAAAAAQIAAAZAEAAAAAAAARIAAAmAEAAAAAAAASIAAAeAEAAAAAAAATIAAAiAEAAAAAAAAgIAAAxAEAAAAAAAAwIAAA8AEAAAAAAAABwAAAOAEAAAAAAAARwAAAFAAAAAAAAAASwAAASAEAAAAAAACA8AAA+AEAAAAAAAAACAANABoAIwBoAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABHA=}}

@article{behav-open-systems,
	annote = {Discusses open systems

Locked behind subscription, not on scihub},
	author = {Jan C. Willems},
	bdsk-color = {3},
	date-added = {2024-11-18 11:28:39 +0700},
	date-modified = {2024-11-30 13:15:00 +0700},
	doi = {10.1109/MCS.2007.906923},
	journal = {IEEE Control Systems},
	keywords = {open systems, behavior, bond graphs},
	month = {January},
	number = {6},
	pages = {46-99},
	title = {The Behavioral Approach to Open and Interconnected Systems},
	volume = {27},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/MCS.2007.906923}}

@article{backpropagation,
	author = {David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
	date-added = {2024-11-17 21:00:48 +0700},
	date-modified = {2024-11-23 16:59:13 +0700},
	journal = {Nature},
	keywords = {neural networks, perceptron-convergence procedure, gradient descent},
	month = {October},
	number = {6088},
	pages = {533-536},
	title = {Learning representations by back-propagating errors},
	volume = {323},
	year = {1986},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEEcuLi9TdHVkeS9QYXBlcnMvTGVhcm5pbmcgcmVwcmVzZW50YXRpb25zIGJ5IGJhY2stcHJvcGFnYXRpbmcgZXJyb3JzLnBkZk8RBEhib29rSAQAAAAABBAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4AwAABQAAAAEBAABVc2VycwAAAAYAAAABAQAAc2NhbGVzAAAFAAAAAQEAAFN0dWR5AAAABgAAAAEBAABQYXBlcnMAADcAAAABAQAATGVhcm5pbmcgcmVwcmVzZW50YXRpb25zIGJ5IGJhY2stcHJvcGFnYXRpbmcgZXJyb3JzLnBkZgAUAAAAAQYAAAQAAAAUAAAAJAAAADQAAABEAAAACAAAAAQDAAClQQAAAAAAAAgAAAAEAwAAKdoFAAAAAAAIAAAABAMAACuYJAAAAAAACAAAAAQDAABzx18AAAAAAAgAAAAEAwAA6otsAgAAAAAUAAAAAQYAAKAAAACwAAAAwAAAANAAAADgAAAACAAAAAAEAABBxnUXZcleQBgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAADAAAAAAAAAAQAAAADAwAA9wEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAJCClucAAAAIAAAAAAQAAEHGvNWFAAAAJAAAAAEBAAA0Mzg3NjU4NC00QjY4LTRFQTMtQjAwMi1FNDRBNEI5MkFENEMYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAAGgAAAAEBAABOU1VSTERvY3VtZW50SWRlbnRpZmllcktleQAABAAAAAMDAAD8OQEABAEAAAECAABjNmNjMGZlOTlmOGM0YjdiMmE0ZjdmYjMwOWQyNjI2MjI1NjczYmQ1NmY1YWMzZjk4N2VlZjI0OTBmOGM4ZjU0OzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMGY7MDAwMDAwMDAwMjZjOGJlYTswMTsvdXNlcnMvc2NhbGVzL3N0dWR5L3BhcGVycy9sZWFybmluZyByZXByZXNlbnRhdGlvbnMgYnkgYmFjay1wcm9wYWdhdGluZyBlcnJvcnMucGRmANgAAAD+////AQAAAAAAAAARAAAABBAAAIQAAAAAAAAABRAAAPAAAAAAAAAAEBAAABwBAAAAAAAAQBAAAAwBAAAAAAAAAiAAAOgBAAAAAAAABSAAAFgBAAAAAAAAECAAAGgBAAAAAAAAESAAAJwBAAAAAAAAEiAAAHwBAAAAAAAAEyAAAIwBAAAAAAAAICAAAMgBAAAAAAAAMCAAAPQBAAAAAAAAAcAAADwBAAAAAAAAEcAAABQAAAAAAAAAEsAAAEwBAAAAAAAAgPAAACwCAAAAAAAA/AEAgCACAAAAAAAAAAgADQAaACMAbQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAS5}}

@inproceedings{cgt,
	abstract = {We introduce open games as a compositional foundation of economic game theory. A compositional approach potentially allows methods of game theory and theoretical computer science to be applied to large-scale economic models for which standard economic tools are not practical. An open game represents a game played relative to an arbitrary environment and to this end we introduce the concept of coutility, which is the utility generated by an open game and returned to its environment. Open games are the morphisms of a symmetric monoidal category and can therefore be composed by categorical composition into sequential move games and by monoidal products into simultaneous move games. Open games can be represented by string diagrams which provide an intuitive but formal visualisation of the information flows. We show that a variety of games can be faithfully represented as open games in the sense of having the same Nash equilibria and off-equilibrium best responses.},
	author = {Neil Ghani and Jules Hedges and Viktor Winschel and Philipp Zahn},
	booktitle = {Proceedings of the 33rd Annual ACM/IEEE Symposium on Logic in Computer Science, {LICS} 2018},
	date-added = {2024-11-17 20:38:10 +0700},
	date-modified = {2025-02-12 19:36:57 +0700},
	doi = {https://doi.org/10.1145/3209108.3209165},
	eprint = {1603.04641},
	isbn = {978-1-4503-5583-4},
	keywords = {compositionality, open games, compositional game theory, economic game theory, string diagrams, categorical composition, nash equilibrium, coutility},
	month = {July},
	pages = {472--481},
	publisher = {Association for Computing Machinery},
	rating = {4},
	read = {1},
	title = {Compositional game theory},
	url = {https://arxiv.org/pdf/1603.04641.pdf},
	url2 = {https://dl.acm.org/doi/10.1145/3209108.3209165},
	year = {2018},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEC0uLi9TdHVkeS9QYXBlcnMvQ29tcG9zaXRpb25hbCBHYW1lIFRoZW9yeS5wZGZPEQQYYm9vaxgEAAAAAAQQMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAMAAAUAAAABAQAAVXNlcnMAAAAGAAAAAQEAAHNjYWxlcwAABQAAAAEBAABTdHVkeQAAAAYAAAABAQAAUGFwZXJzAAAdAAAAAQEAAENvbXBvc2l0aW9uYWwgR2FtZSBUaGVvcnkucGRmAAAAFAAAAAEGAAAEAAAAFAAAACQAAAA0AAAARAAAAAgAAAAEAwAApUEAAAAAAAAIAAAABAMAACnaBQAAAAAACAAAAAQDAAArmCQAAAAAAAgAAAAEAwAAc8dfAAAAAAAIAAAABAMAAJ3F9QEAAAAAFAAAAAEGAACIAAAAmAAAAKgAAAC4AAAAyAAAAAgAAAAABAAAQcZ1Fd87mRYYAAAAAQIAAAEAAAAAAAAADwAAAAAAAAAAAAAAAAAAAAgAAAAEAwAAAwAAAAAAAAAEAAAAAwMAAPcBAAAIAAAAAQkAAGZpbGU6Ly8vDAAAAAEBAABNYWNpbnRvc2ggSEQIAAAABAMAAACQgpbnAAAACAAAAAAEAABBxrzVhQAAACQAAAABAQAANDM4NzY1ODQtNEI2OC00RUEzLUIwMDItRTQ0QTRCOTJBRDRDGAAAAAECAACBAAAAAQAAAO8TAAABAAAAAAAAAAAAAAABAAAAAQEAAC8AAAAAAAAAAQUAABoAAAABAQAATlNVUkxEb2N1bWVudElkZW50aWZpZXJLZXkAAAQAAAADAwAAjTkBAOoAAAABAgAAOTE3YjdlOTNlODU4OWFkOWQzZTVjNTVkNDZhOTQyZGFkYzQ4NzdiMDQ5MWQ0ODgyZTAxYTM1MjA0ZDYzMmU3YzswMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDswMDAwMDAwMDAwMDAwMDIwO2NvbS5hcHBsZS5hcHAtc2FuZGJveC5yZWFkLXdyaXRlOzAxOzAxMDAwMDBmOzAwMDAwMDAwMDFmNWM1OWQ7MDE7L3VzZXJzL3NjYWxlcy9zdHVkeS9wYXBlcnMvY29tcG9zaXRpb25hbCBnYW1lIHRoZW9yeS5wZGYAAADYAAAA/v///wEAAAAAAAAAEQAAAAQQAABsAAAAAAAAAAUQAADYAAAAAAAAABAQAAAEAQAAAAAAAEAQAAD0AAAAAAAAAAIgAADQAQAAAAAAAAUgAABAAQAAAAAAABAgAABQAQAAAAAAABEgAACEAQAAAAAAABIgAABkAQAAAAAAABMgAAB0AQAAAAAAACAgAACwAQAAAAAAADAgAADcAQAAAAAAAAHAAAAkAQAAAAAAABHAAAAUAAAAAAAAABLAAAA0AQAAAAAAAIDwAAAUAgAAAAAAAOQBAIAIAgAAAAAAAAAIAA0AGgAjAFMAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAAEbw==},
	bdsk-file-2 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEGouLi9TdHVkeS9QYXBlcnMvQSBDb21wcmVoZW5zaXZlIEJlbmNobWFyayBvZiBNYWNoaW5lIGFuZCBEZWVwIExlYXJuaW5nIEFjcm9zcyBEaXZlcnNlIFRhYnVsYXIgRGF0YXNldHMucGRmTxEEVGJvb2tUBAAAAAAEEDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFADAAAFAAAAAQEAAFVzZXJzAAAABgAAAAEBAABzY2FsZXMAAAUAAAABAQAAU3R1ZHkAAAAGAAAAAQEAAFBhcGVycwAAWgAAAAEBAABBIENvbXByZWhlbnNpdmUgQmVuY2htYXJrIG9mIE1hY2hpbmUgYW5kIERlZXAgTGVhcm5pbmcgQWNyb3NzIERpdmVyc2UgVGFidWxhciBEYXRhc2V0cy5wZGYAABQAAAABBgAABAAAABQAAAAkAAAANAAAAEQAAAAIAAAABAMAAKVBAAAAAAAACAAAAAQDAAAp2gUAAAAAAAgAAAAEAwAAK5gkAAAAAAAIAAAABAMAAHPHXwAAAAAACAAAAAQDAACJzCQCAAAAABQAAAABBgAAxAAAANQAAADkAAAA9AAAAAQBAAAIAAAAAAQAAEHGjK0v+sxFGAAAAAECAAABAAAAAAAAAA8AAAAAAAAAAAAAAAAAAAAIAAAABAMAAAMAAAAAAAAABAAAAAMDAAD3AQAACAAAAAEJAABmaWxlOi8vLwwAAAABAQAATWFjaW50b3NoIEhECAAAAAQDAAAAkIKW5wAAAAgAAAAABAAAQca81YUAAAAkAAAAAQEAADQzODc2NTg0LTRCNjgtNEVBMy1CMDAyLUU0NEE0QjkyQUQ0QxgAAAABAgAAgQAAAAEAAADvEwAAAQAAAAAAAAAAAAAAAQAAAAEBAAAvAAAAAAAAAAEFAAAnAQAAAQIAAGU5NDUxOGQ2MDE3ZmQyNDQ1NGNiMDYzM2NkOWMxNjgxNDIyYWVlMGIxNjljOWNiYjQ4NGQ4M2FiZTIxODZmZmU7MDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDAwMDAwMDAyMDtjb20uYXBwbGUuYXBwLXNhbmRib3gucmVhZC13cml0ZTswMTswMTAwMDAwZjswMDAwMDAwMDAyMjRjYzg5OzAxOy91c2Vycy9zY2FsZXMvc3R1ZHkvcGFwZXJzL2EgY29tcHJlaGVuc2l2ZSBiZW5jaG1hcmsgb2YgbWFjaGluZSBhbmQgZGVlcCBsZWFybmluZyBhY3Jvc3MgZGl2ZXJzZSB0YWJ1bGFyIGRhdGFzZXRzLnBkZgAAzAAAAP7///8BAAAAAAAAABAAAAAEEAAAqAAAAAAAAAAFEAAAFAEAAAAAAAAQEAAAQAEAAAAAAABAEAAAMAEAAAAAAAACIAAADAIAAAAAAAAFIAAAfAEAAAAAAAAQIAAAjAEAAAAAAAARIAAAwAEAAAAAAAASIAAAoAEAAAAAAAATIAAAsAEAAAAAAAAgIAAA7AEAAAAAAAAwIAAAGAIAAAAAAAABwAAAYAEAAAAAAAARwAAAFAAAAAAAAAASwAAAcAEAAAAAAACA8AAAIAIAAAAAAAAACAANABoAIwCQAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAABOg=},
	bdsk-url-1 = {https://arxiv.org/pdf/1603.04641.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1603.04641}}

@phdthesis{towards-cgt,
	abstract = {We introduce a new foundation for game theory based on so-called open games. Unlike existing approaches open games are fully compositional: games are built using algebraic operations from standard components, such as players and outcome functions, with no fundamental distinction being made between the parts and the whole. Open games are intended to be applied at large scales where classical game theory becomes impractical to use, and this thesis therefore covers part of the theoretical foundation of a powerful new tool for economics and other subjects using game theory.
Formally we define a symmetric monoidal category whose morphisms are open games, which can therefore be combined either sequentially using categorical composition, or simultaneously using the monoidal product. Using this structure we can also graphically represent open games using string diagrams. We prove that the new definitions give the same results (both equilibria and off-equilibrium best responses) as classical game theory in several important special cases: normal form games with pure and mixed strategy Nash equilibria, and perfect information games with subgame perfect equilibria.
This thesis also includes work on higher order game theory, a related but simpler approach to game theory that uses higher order functions to model players. This has been extensively developed by Martin Escard ́o and Paulo Oliva for games of perfect information, and we extend it to normal form games. We show that this approach can be used to elegantly model coordination and differentiation goals of players. We also argue that a modification of the solution concept used by Escard ́o and Oliva is more appropriate for such applications.},
	annote = {Has CGT proofs},
	author = {Julian Hedges},
	date-added = {2024-11-17 20:27:13 +0700},
	date-modified = {2025-02-10 16:44:07 +0700},
	keywords = {compositional game theory, open games, higher order game theory, games of perfect infromation, normal form games},
	school = {Queen Mary University of London},
	title = {Towards compositional game theory},
	year = {2016},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEDUuLi9TdHVkeS9QYXBlcnMvVG93YXJkcyBDb21wb3NpdGlvbmFsIEdhbWUgVGhlb3J5LnBkZk8RA+xib29r7AMAAAAABBAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADoAgAABQAAAAEBAABVc2VycwAAAAYAAAABAQAAc2NhbGVzAAAFAAAAAQEAAFN0dWR5AAAABgAAAAEBAABQYXBlcnMAACUAAAABAQAAVG93YXJkcyBDb21wb3NpdGlvbmFsIEdhbWUgVGhlb3J5LnBkZgAAABQAAAABBgAABAAAABQAAAAkAAAANAAAAEQAAAAIAAAABAMAAKVBAAAAAAAACAAAAAQDAAAp2gUAAAAAAAgAAAAEAwAAK5gkAAAAAAAIAAAABAMAAHPHXwAAAAAACAAAAAQDAACiD4wCAAAAABQAAAABBgAAkAAAAKAAAACwAAAAwAAAANAAAAAIAAAAAAQAAEHGdRNBlGEGGAAAAAECAAABAAAAAAAAAA8AAAAAAAAAAAAAAAAAAAAIAAAABAMAAAMAAAAAAAAABAAAAAMDAAD3AQAACAAAAAEJAABmaWxlOi8vLwwAAAABAQAATWFjaW50b3NoIEhECAAAAAQDAAAAkIKW5wAAAAgAAAAABAAAQca81YUAAAAkAAAAAQEAADQzODc2NTg0LTRCNjgtNEVBMy1CMDAyLUU0NEE0QjkyQUQ0QxgAAAABAgAAgQAAAAEAAADvEwAAAQAAAAAAAAAAAAAAAQAAAAEBAAAvAAAAAAAAAAEFAADyAAAAAQIAAGU1OWRhMDgwMDVjOTZhY2FmMGVhZTFjOTA2OGZlNDkyYTA0OGU1YjgyYzQzYzUxM2ZmNTZmMDZiYmI4ZDViZWY7MDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDAwMDAwMDAyMDtjb20uYXBwbGUuYXBwLXNhbmRib3gucmVhZC13cml0ZTswMTswMTAwMDAwZjswMDAwMDAwMDAyOGMwZmEyOzAxOy91c2Vycy9zY2FsZXMvc3R1ZHkvcGFwZXJzL3Rvd2FyZHMgY29tcG9zaXRpb25hbCBnYW1lIHRoZW9yeS5wZGYAAADMAAAA/v///wEAAAAAAAAAEAAAAAQQAAB0AAAAAAAAAAUQAADgAAAAAAAAABAQAAAMAQAAAAAAAEAQAAD8AAAAAAAAAAIgAADYAQAAAAAAAAUgAABIAQAAAAAAABAgAABYAQAAAAAAABEgAACMAQAAAAAAABIgAABsAQAAAAAAABMgAAB8AQAAAAAAACAgAAC4AQAAAAAAADAgAADkAQAAAAAAAAHAAAAsAQAAAAAAABHAAAAUAAAAAAAAABLAAAA8AQAAAAAAAIDwAADsAQAAAAAAAAAIAA0AGgAjAFsAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAAESw==}}

@book{automl-mcs,
	author = {Frank Hutter},
	date-added = {2024-11-14 18:44:43 +0700},
	date-modified = {2024-11-14 18:52:23 +0700},
	keywords = {machine learning, automl, automation},
	publisher = {Springer},
	title = {Automated Machine Learning: Methods, Systems, Challenges},
	year = {2018}}

@article{Fan_2024,
	abstract = {This paper investigates the stability of deep ReLU neural networks for nonparametric regression under the assumption that the noise has only a finite pth moment. We unveil how the optimal rate of convergence depends on p, the degree of smoothness and the intrinsic dimension in a class of nonparametric regression functions with hierarchical composition structure when both the adaptive Huber loss and deep ReLU neural networks are used. This optimal rate of convergence cannot be obtained by the ordinary least squares but can be achieved by the Huber loss with a properly chosen parameter that adapts to the sample size, smoothness, and moment parameters. A concentration inequality for the adaptive Huber ReLU neural network estimators with allowable optimization errors is also derived. To establish a matching lower bound within the class of neural network estimators using the Huber loss, we employ a different strategy from the traditional route: constructing a deep ReLU network estimator that has a better empirical loss than the true function and the difference between these two functions furnishes a low bound. This step is related to the Huberization bias, yet more critically to the approximability of deep ReLU networks. As a result, we also contribute some new results on the approximation theory of deep ReLU neural networks.},
	author = {Fan, Jianqing and Gu, Yihong and Zhou, Wen-Xin},
	date-added = {2024-11-14 18:38:05 +0700},
	date-modified = {2024-11-14 18:53:20 +0700},
	doi = {10.1214/24-aos2428},
	issn = {0090-5364},
	journal = {The Annals of Statistics},
	keywords = {approximablility of ReLU networks , composition of functions , heavy tails , Optimal rates , robustness , truncation, machine learning},
	month = aug,
	number = {4},
	publisher = {Institute of Mathematical Statistics},
	title = {How do noise tails impact on deep ReLU networks?},
	url = {http://dx.doi.org/10.1214/24-AOS2428},
	volume = {52},
	year = {2024},
	bdsk-url-1 = {http://dx.doi.org/10.1214/24-AOS2428}}

@article{hall2023,
	abstract = {The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), leading to remarkable advancements in text understand- ing and generation. Nevertheless, alongside these strides, LLMs exhibit a critical tendency to produce hallucinations, resulting in content that is inconsistent with real-world facts or user inputs. This phenomenon poses substantial challenges to their practical deployment and raises concerns over the reliability of LLMs in real-world scenarios, which attracts increasing attention to detect and mitigate these hallucina- tions. In this survey, we aim to provide a thor- ough and in-depth overview of recent advances in the field of LLM hallucinations. We begin with an innovative taxonomy of LLM halluci- nations, then delve into the factors contributing to hallucinations. Subsequently, we present a comprehensive overview of hallucination de- tection methods and benchmarks. Additionally, representative approaches designed to mitigate hallucinations are introduced accordingly. Fi- nally, we analyze the challenges that highlight the current limitations and formulate open ques- tions, aiming to delineate pathways for future research on hallucinations in LLMs. },
	author = {Lei Huang and Weijiang Yu and Weitao Ma and Weihong Zhong and Zhangyin Feng and Haotian Wang and Qianglong Chen and Weihua Peng and Xiaocheng Feng and Bing Qin and Ting Liu},
	date-added = {2024-11-14 11:11:20 +0700},
	date-modified = {2024-11-18 11:59:37 +0700},
	keywords = {nlp, llm, hallucination, survey},
	rating = {3},
	read = {0},
	title = {Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions},
	year = {2023},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEIouLi9TdHVkeS9QYXBlcnMvWmVybyBGb3JtdWxhZS9BIFN1cnZleSBvbiBIYWxsdWNpbmF0aW9uIGluIExhcmdlIExhbmd1YWdlIE1vZGVscy0gUHJpbmNpcGxlcywgVGF4b25vbXksIENoYWxsZW5nZXMsIGFuZCBPcGVuIFF1ZXN0aW9ucy5wZGZPEQS0Ym9va7QEAAAAAAQQMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAsAMAAAUAAAABAQAAVXNlcnMAAAAGAAAAAQEAAHNjYWxlcwAABQAAAAEBAABTdHVkeQAAAAYAAAABAQAAUGFwZXJzAAANAAAAAQEAAFplcm8gRm9ybXVsYWUAAABsAAAAAQEAAEEgU3VydmV5IG9uIEhhbGx1Y2luYXRpb24gaW4gTGFyZ2UgTGFuZ3VhZ2UgTW9kZWxzLSBQcmluY2lwbGVzLCBUYXhvbm9teSwgQ2hhbGxlbmdlcywgYW5kIE9wZW4gUXVlc3Rpb25zLnBkZhgAAAABBgAABAAAABQAAAAkAAAANAAAAEQAAABcAAAACAAAAAQDAAClQQAAAAAAAAgAAAAEAwAAKdoFAAAAAAAIAAAABAMAACuYJAAAAAAACAAAAAQDAABzx18AAAAAAAgAAAAEAwAAUoC/AQAAAAAIAAAABAMAAIrOdAAAAAAAGAAAAAEGAADwAAAAAAEAABABAAAgAQAAMAEAAEABAAAIAAAAAAQAAEHGRi3KsoGiGAAAAAECAAABAAAAAAAAAA8AAAAAAAAAAAAAAAAAAAAIAAAABAMAAAQAAAAAAAAABAAAAAMDAAD3AQAACAAAAAEJAABmaWxlOi8vLwwAAAABAQAATWFjaW50b3NoIEhECAAAAAQDAAAAkIKW5wAAAAgAAAAABAAAQca81YUAAAAkAAAAAQEAADQzODc2NTg0LTRCNjgtNEVBMy1CMDAyLUU0NEE0QjkyQUQ0QxgAAAABAgAAgQAAAAEAAADvEwAAAQAAAAAAAAAAAAAAAQAAAAEBAAAvAAAAAAAAAAEFAABHAQAAAQIAAGIwYjAzMWY3OGYzNTJjMWIwZTIzNGI3MGQ3YjY5NWQyMjczYjI4ZGE4OTUyMDUyYzlhMDA3OGE1YTY4ZWU4N2I7MDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDA7MDAwMDAwMDAwMDAwMDAyMDtjb20uYXBwbGUuYXBwLXNhbmRib3gucmVhZC13cml0ZTswMTswMTAwMDAwZjswMDAwMDAwMDAwNzRjZThhOzAxOy91c2Vycy9zY2FsZXMvc3R1ZHkvcGFwZXJzL3plcm8gZm9ybXVsYWUvYSBzdXJ2ZXkgb24gaGFsbHVjaW5hdGlvbiBpbiBsYXJnZSBsYW5ndWFnZSBtb2RlbHMtIHByaW5jaXBsZXMsIHRheG9ub215LCBjaGFsbGVuZ2VzLCBhbmQgb3BlbiBxdWVzdGlvbnMucGRmAADMAAAA/v///wEAAAAAAAAAEAAAAAQQAADQAAAAAAAAAAUQAABQAQAAAAAAABAQAACAAQAAAAAAAEAQAABwAQAAAAAAAAIgAABMAgAAAAAAAAUgAAC8AQAAAAAAABAgAADMAQAAAAAAABEgAAAAAgAAAAAAABIgAADgAQAAAAAAABMgAADwAQAAAAAAACAgAAAsAgAAAAAAADAgAABYAgAAAAAAAAHAAACgAQAAAAAAABHAAAAUAAAAAAAAABLAAACwAQAAAAAAAIDwAABgAgAAAAAAAAAIAA0AGgAjALAAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAAFaA==}}

@comment{BibDesk Static Groups{
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<array>
	<dict>
		<key>group name</key>
		<string>CGTNN</string>
		<key>keys</key>
		<string>tseng20,cgt,towards-cgt,romano2021pmlb,leelim20,decprop-nn,AmosKolter2017,pnnucoopgt,sevensketches,scikit-learn,dlpl</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>Nawkas</string>
		<key>keys</key>
		<string>maml2017,reptile2018,AmosKolter2017</string>
	</dict>
</array>
</plist>
}}
